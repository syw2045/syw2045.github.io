---
title: "[DL] CS231n Lecture 7: Training Neural Networks, Part II"

categories:
  - DL
tags:
  - [DL, CV]

toc: true
toc_sticky: true

date: 2025-02-19
last_modified_at: 2025-02-19
---

# 1. Fancier optimization

## 1.1 SGD

Network에서 weight에 대한 Loss function을 정의해 두면 Loss function은 Weight가 좋은지 안좋은지 판단한다.

![Image](https://github.com/user-attachments/assets/96e7e25d-e303-43f1-8280-1455866117c6)

앞서 배운 가장 간단한 최적화(optimization)기법은 Stochastic Gradient Descent(SGD)이다. 하지만 SGD는 몇가지 문제점을 가지고 있다.

![Image](https://github.com/user-attachments/assets/1d91ba92-b81d-41d1-b5e8-1a1d8f0d525d)

수평축의 가중치는 변해도 Loss가 아주 천천히 줄어든다. 즉, Loss는 수직 방향의 가중치 변화에 훨씬 더 민감하게 반응한다.

현재 지점에서 Loss는 bad condition number를 가지고 있다고 할 수 있다. 이 지점의 hessian matrix의 최대/최소 singular value ratio가 매우 안좋다.

> Local minima and saddle point

![Image](https://github.com/user-attachments/assets/0b8be719-99b9-4968-870b-de7072ca43e8){: width="20%", height="30%"} 

1차원 공간에서는 local minima가 자주 발생하고 좋지 않은 상황이라고 생각할 수 있지만 고차원으로 갔을 때는 saddle point가 훨씬 빈번하게 일어난다.

예를들어 1억개의 차원에서 local minima는 1억개의 방향을 다계산했는데 모두 Loss가 증가하는것 밖에 없는 것인데 이런 경우는 드물다. 반면 saddle point는 어떤 방향으로는 Loss가 증가하는데 몇몇 방향으로는 Loss가 감소하는 경우이다. 이러한 경우는 생각보다 더 빈번하게 일어난다. 

또한 saddle point자체도 문제지만, 그 주변에서도 문제가 발생한다. saddle point 근처에서는 gradient가 0은 아니지만 매우 작기 때문에 업데이트가 매우 느리게 진행된다.

학습데이터의 경우 N은 백만이 될 수도 있다. 손실함수를 계산할 때 엄청 많은 학습 데이터 각각의 Loss를 계산해야 한다. 하지만 이를 전부계산하는 것은 어렵기 때문에 mini batch의 데이터들만 가지고 실제 Loss를 추정한다. 이는 매번 정확한 gradient를 얻을 수가 없다는 것을 의미한다. 대신 gradient의 noisy estimate를 구할 뿐이다.

## 1.2 momentum SGD

![Image](https://github.com/user-attachments/assets/67b3b08f-f808-4e63-99eb-9402372ebce4)

momentum SGD은 gradient를 계산할 때 velocity를 이용한다. 즉, 현재 미니배치의 gradient 방향만 고려하는 것이 아니라 velocity도 같이 고려하는 것이다. rho라는 하이퍼파라미터를 통해 velocity를 조절할 수 있다.

local minima에 도달해도 여전히 velocity를 가지고 있기 때문에 gradient가 0이어도 움직일 수 있다.

saddle point에서도 주변의 gradient가 작더라도, 굴러내려오는 속도가 있기 때문에 velocity를 갖게 된다.

이를 통해 Loss에 민감한 수직 방향의 변동은 줄여주고, 수평방향의 움직임은 점차 가속화 된다. momentum을 추가해서 velocity가 생기면 결국 noise가 평균화되버린다.

![Image](https://github.com/user-attachments/assets/3a142d84-196e-42ec-b730-3283e7b97b1a)

Gradient와 Velocity의 합으로 actual step을 이동하게 되고 이는 noise를 줄여주는 역할을 한다.

## 1.3 Nesterov Momentum

![Image](https://github.com/user-attachments/assets/446968d5-f5a8-4b60-93e4-38fa91690ed0)

SGD Momentum은 현재 지점에서 Gradient를 계산하고 Velocity를 계산해 Gradient를 구하지만 Nesterov Momentum은 Velocity를 구해 이동한 뒤 그 지점에서 Gradient를 구하고 다시 원점으로 이동해 Velocity와 이동한 지점에서 구한 Gradient를 더한다.

이는 Velocity의 방향이 잘못되었을 경우에 현재 gradient 방향을 좀 더 활용할 수 있도록 한다.

Nesterov는 convex optimization문제는 뛰어난 성능을 보이지만 Neural network같은 non-convex problem에서는 성능을 보장하지 못한다. 

![Image](https://github.com/user-attachments/assets/b8872a4e-5e77-4286-82e3-ae6a578718d4)

Nesterov의 원래 수식은 Loss와 gradient를 다른 지점에서 구하는 방식이다. 하지만 이 식의 변수를 약간만 변형을 해주면 같은 지점에서 계산할 수 있다.

Nesterov momentum은 현재/이전의 velocity간의 error-correcting term이 추가됐다.

## 1.4 AdaGrad

![Image](https://github.com/user-attachments/assets/7ef1bc35-986d-41bf-8c88-49a94f52723d)

AdaGrad는 velocity term 대신에 grad squared term을 이용한다. 그리고 학습 도중에 계산되는 gradient에 제곱을 해서 계속 더해준다. 그리고 update할 때 update term을 gradient로 나눠준다.

(1e-7 : 분모가 0이 아니게 해주는 상수, 큰 영향력은 없다)

![Image](https://github.com/user-attachments/assets/db365d3e-4b62-4c9f-a3c0-0057ec9d5a50)

AdaGrad를 사용하면 Small dimension에서는 gradient의 제곱의 합이 작다. 그래서 작은 값이 나눠지게 되므로 큰 가속도를 갖는다. 반면 Large dimension에서는 큰 값이 나눠지므로 속도가 점점 줄어들게 된다.

학습 횟수 t가 계속 늘어나면 grad_squared누적값이 점점 커지게 되므로 step size가 점점 작아지게 된다. 이것은 convex case의 경우 장점이 될 수 있겠지만, non-convex case는 saddle point에서 AdaGrad가 멈춰버릴 수 있다.

일반적으로 Neural Net학습시킬때 잘 사용하지는 않는다.

## 1.5 RMSProp

![Image](https://github.com/user-attachments/assets/4f273502-1d3b-419d-a07e-ca95557e64da)

앞서 언급한 AdaGrad의 문제점을 개선한 방법이다.

grad_squared를 구하지만 앞에 decay_rate를 곱해 누적시키는 방법이다. (보통 0.9 혹은 0.99 사용)

이를 통해 AdaGrad의 점점 속도가 줄어드는 문제를 해결할 수 있다.

## 1.6 Adam

![Image](https://github.com/user-attachments/assets/4ebced1a-9c14-4f23-bd45-35da28db3e52)

Bias correction을 추가해 first/second moments를 update하고 난 후 현재 Step에 맞는 적절한 unbiased term을 계산한다.

![Image](https://github.com/user-attachments/assets/2ec5fed8-c263-4280-b147-82434535b552){: width="40%", height="60%"}

지금까지 배운 optimizer들을 보면 위와같이 나타낼 수 있다.

![Image](https://github.com/user-attachments/assets/24a869fc-cf96-41bb-af5d-dd0f6100e86b)

Learning rate decay는 Adam보다는 SGD Momentum에서 자주 사용된다. 그리고 그것은 second-order 하이퍼파라미터이다. 일반적으로 학습초기부터 고려하는 것은 아니다. 학습 초기에는 dacay가 없다고 생각하고 learning rate 자체를 잘 설정하는 것이 더 중요하다.

## 1.7 Newton methods

![Image](https://github.com/user-attachments/assets/55d5f051-10df-4737-92a9-271f62b16379){: width="40%", height="60%"} ![Image](https://github.com/user-attachments/assets/5a78f8ca-f257-4de4-9096-c26e55822985){: width="40%", height="60%"}

지금까지 배운 것은  linear approximation(1차 함수)를 통해 gradient를 계산하는 방식이었다.(First-Order Optimization) 하지만 이것을 second-order approximation의 정보를 사용하는 방법을 사용하면 minima에 더 잘 접근할 수 있다.

이것을 다차원으로 확장시키면 Newton step이라고 한다.

![Image](https://github.com/user-attachments/assets/93634909-9197-42a3-96ed-950a96a7427d){: width="40%", height="60%"}

이 방식은 minima로 이동하는게 아니라 minima의 방향으로 이동하기 때문에 기본버전에서는 Learning rate가 없다. 하지만 이것은 딥러닝에서 사용할 수가 없다 왜냐하면 Hessian을 계산하는데 O(N^2)가 걸리는데 N은 파라미터 수 이다. 예를들어 1억개라고 했을때 계산을 하기에는 너무 크다.

그래서 실제로는 Quasi-Newton methods(BGFS)를 사용한다. full hessian을 계산하는 것이 아니라 Low-rank approximations를 통해 근사시켜서 사용한다.  

![Image](https://github.com/user-attachments/assets/f4ae7175-ff26-4196-808e-aca6cd919459)

지금까지 배운 Optimization 알고리즘들은 training error를 줄이고 손실함수를 최소화하기 위해 사용했다. 하지만 사실 training error에 대해선 큰 관심이 없고 한번도 보지 못한 데이터에 대한 성능이 더 중요하다.

우리가 원하는 것은 train/test error의 격차를 줄이는 것이다.

격차를 줄이는 가장 쉬운 방법은 Model ensemble이다.

Model ensemble은 각각의 독립적인 모델을 학습한뒤 평균을 사용하는 것이다. 모델의 수가 늘어날 수록 overfitting이 줄어들고 성능이 조금씩(보통 2%) 향상된다.

# 2. Regularization

Essemble이 아닌 single-model의 성능을 향상시키는 방법 중 하나는 Regularization이다.

Essenble은 Test time동안 여러개의 모델을 학습해야하기 때문에 좋지 않을 수 도 있다.  regularization을 통해 모델이 training data에 fit하는 것을 막아주고 한번도 보지 못한 데이터에 대한 성능을 향상시킬 수도 있다.

## 2.1 Dropout

Nerual Net에서 가장 많이 사용하는 regularization은 Dropout이다.

![Image](https://github.com/user-attachments/assets/4740fa04-752e-4839-9149-1c177c80928d)

dropout은 forward pass 과정에서 임의로 일부 뉴런을 0으로 만드는 것이다. forward pass할 때마다 0이 되는 뉴런이 바뀌고 한 Layer마다 Dropout이 진행된다.

Dropout이 좋은 성능을 내는데에 대한 여러 해석이 있다.

![Image](https://github.com/user-attachments/assets/abb2d346-6337-4749-934a-e4913b9bd2cd)

첫번째는 feature간의 co-adaptation을 방지한다는 것이다.

또 다른 해석은 임의의 뉴런이 0이 되면서 매번 새로운 sub-network가 만들어진다 그러므로 Dropout이 파라미터를 공유하는 거대한 ensemble model을 학습하는 것이다. 

## 2.2 Test Time

예를들어 오늘은 고양이 이미지를 고양이라고 분류했지만 내일은 다른 이미지로 분류하는 것 같이 이미 학습된 Network의 Test time에는 Stochasticity(임의성)는 적합하지 않다. 대신 그러한 임의성(randomness)를 average out시킨다.

![Image](https://github.com/user-attachments/assets/4c4e2518-91e7-466c-9d54-cb4c1d812047)

이를 위해 위 적분수식을 계산하면 average out시킬 수 있다. 하지만 이 적분 식을 계산하는 것이 복잡하다. 이 적분 식을 구하는 것이 아니라 approximate할 수 있다.

![Image](https://github.com/user-attachments/assets/b5398f5e-e362-45c0-99d1-380dee43a10a)

test time과 training time에서의 기댓값 차이는 0.5배이다. Test time에서 Stochasticity를 사용하지 않고 할 수 있는 가장 Cheap한 방법중 하나는 dropout probability를 Network 출력에 곱하는 것이다. 

Training시에는 randomness를 추가해 학습 데이터에 너무 fit하지 않게 해주고, Test시에는 그것을 average out을 통해 일반화시켜준다. 이러한 방식은 Batch Normalization과 유사한 방식이다.

BN에서도 training time에서는 각 데이터를 얼마나 normalization할지에 대한 stochasticity가 존재했다. 하지만 test time에서는 normalization을 batch 단위가 아닌 global 단위로 진행해 stochasticity를 평균화 시킨다. 

BN과 유사한 효과를 내는 dropout을 그럼에도 사용하는 이유는 우리가 조절할 수 있는 파라미터 p가 있기 때문이다.

## 2.3 Data Augmentation

![Image](https://github.com/user-attachments/assets/005c5dd7-5472-4261-a82d-8f4b93b9bc7c)

이러한 regularization 패러다임에 부합하는 전략중 하나는 Data augmentation이다.

data augmentation을 통해 무작위로 학습을 시킨다. 예를 들어 좌우 반전을 시킨다고 해도 Cat 이미지는 좌우만 바뀐 Cat이다. 그러므로 randomness를 학습시에 사용을 해도 Test시에는 일반화가 가능하다.

Data Augmentation은 다양한 방법을 통해 할 수 있다.

- Random crops and scales
- Color Jitter (contrast and brightness)
- translation
- rotation
- stretching
- shearing

# 3. Transfer Learning

![Image](https://github.com/user-attachments/assets/9e3b7539-a9ce-4e19-a613-e3357b853dd9)

먼저 ImageNet을 통해 Model을 학습시킨다. 그다음 작은 Dataset을 가지고 C classese를 분류한다. 1000개의 ImageNet 카테고리를 분류하는 것이 아니라 10종의 강아지를 분류하는 문제이다. 가장 마지막 FC Layer는 최종 feature와 class score간의 연결인데 이를 초기화한다. 나머지 부분은 그대로 두고 데이터를 학습한다.

이렇게 되면 linear classfier를 학습시키는 것과 같게된다. 이 방법을 이용하면 데이터 셋이 적더라도 잘 학습시킬 수 있다. 만약 데이터셋이 더 있다면 전체 Network를 fine tuning할 수도 있다. 이때는 보통 사용하는 learning rate보다 1/10정도로 사용한다.

![Image](https://github.com/user-attachments/assets/ccd7e03c-046c-4b31-bea6-10d5198beb8c)

매우 다른 이미지를 가지고 있을 때는 문제가 될 수도 있다. 예를 들면 X-ray이미지 dataset을 매우 적게 가지고 있다면 다른 단계에서 시도를 해봐야할 수도 있다. 그래도 데이터셋이 많다면 여러 Layers를 finetune해서 학습할 수 있다.

![Image](https://github.com/user-attachments/assets/1f1c83c4-c855-4b09-be87-2379a4c91b14)

대부분 pretrained model를 기본으로 하고 사용하는 사람의 상황에 맞게 학습시킨다고 한다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
