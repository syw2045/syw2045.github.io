---
title: "[DL] CS231n Lecture 11: Detection and Segmentation"

categories:
  - DL
tags:
  - [DL, CV, Detection, Segmentation]

toc: true
toc_sticky: true

date: 2025-02-23
last_modified_at: 2025-02-23
---

# 1. Segentation

## 1.1 Semantic Segmentation

![Image](https://github.com/user-attachments/assets/4f3e65c6-1110-4065-ada3-566b66d81ef8)

Semantic Segmentation 문제는 입력은 이미지이고 출력으로 이미지의 모든 픽셀에 카테고리를 정한다.

Classification과의 차이점은 이미지 전체에 대한 카테고리가 아니라 모든 픽셀에 대해서 카테고리화 한다는 것이다.

또한 semantic segmentation은 개별 객체를 구분하지 않는다. (Cow)

## 1.2 Sliding Window

![Image](https://github.com/user-attachments/assets/22294a18-717a-4454-9839-63762fb162c4)

Semantic segmentation은 Sliding Window를 통해 먼저 접근해 볼 수 있다. 먼저 입력 이미지를 아주 작은 단위로 쪼개고 그 영역에 대한 Classification 문제를 푼다고 생각해볼 수 있다.

하지만 이런 방식은 비용이 엄청 크기 때문에 좋은 접근 방식이 아니다. 모든 픽셀에 대해서 작은 영역으로 쪼개고, 이 모든 영역을 forward/backward pass하는 일은 상당히 비효율적이다.

## 1.3 Fully Convolution Network

![Image](https://github.com/user-attachments/assets/a6adca71-fb71-4988-b2ac-12f0283b85f2)

조금 더 개선된 방식이 Fully Convolutional Network이다.

zero padding 같은 것을 적용한 후 Conv Layer를 쌓으면 이미지의 공간 정보를 손실하지 않고 유지할 수 있다. 이 네트워크의 출력 텐서는 C x H x W 이며, 여기서 C는 카테고리 수를 의미한다.
즉, 입력 이미지의 모든 픽셀에 대해 Classification Score를 매긴 결과이다.

학습 과정에서는 모든 픽셀의 Classification Loss를 계산한 후 평균을 취하고, 이를 기반으로 backpropagation을 수행한다.

하지만 이 방식은 이미지의 Spatial Size를 유지하기 때문에 연산 비용이 크다는 단점이 있다. 특히, 고해상도 이미지를 입력하면 계산량과 메모리 소모가 급격히 증가하여 부담이 커진다.

![Image](https://github.com/user-attachments/assets/d0237376-8b0c-4673-b6f7-ac6cf161e270)

대신 feature map을 downsampling/upsampling을 한다.

spatial resolution 전체를 가지고 Convolution을 수행하기 보다는 Original Resolution에서는 Conv layer는 소량만 사용한다. 그리고 Max pooling, Stride Convolution 등으로 feature map을 downsampling 한다.

일반적인 Image Classification에서는 FC Layer를 사용하지만, 이 방식은 Spatial Resolution을 다시 키워 최종적으로 입력 이미지와 동일한 해상도로 복원한다. 이를 통해 계산 효율을 높일 수 있다.

이 방법을 통해 네트워크가 Lower resolution의 feature를 처리하도록 하여 네트워크를 더 깊게 만들 수 있다. 

최종적으로 모든 픽셀에 대한 cross-entropy를 계산하면 네트워크 전체를 end-to-end로 학습시킬 수 있다.

Upsampling은 다양한 방식이 있다.

![Image](https://github.com/user-attachments/assets/04acd548-57cf-40a9-88a2-3a35034eb5e3)

![Image](https://github.com/user-attachments/assets/2f6b30f7-e555-42fa-8bd6-b5aa058ec7f9)

특히 보통의 네트워크는 downsampling/upsampling 비율이 비슷하다.
Max unpooling은 receptive field에서 Max pooling을 했던 위치를 기억해서 다시 upsampling 해주는 것이다.

이 방식들은 고정된 함수의 방식이다. 하지만 Strided convolution은 어떤 식으로 Downsampling을 해야할지 네트워크가 학습할 수 있다. 이와 유사하게 Upsampling에서도 학습가능한 방법이 있다.

Transpose convolution

![Image](https://github.com/user-attachments/assets/b378d7b7-a7a5-4b79-a053-aca2ffb98496)

Transpose convolution 간에 Receptive Field가 겹칠 수 있다. 이때는 단순히 값을 더해주면 된다.

Spatial size를 키워주기 위해서 학습된 필터 가중치를 이용한다.

1D Example

![Image](https://github.com/user-attachments/assets/2d7515f0-f5a3-4469-9646-55c963dba312)

input a,b에 대해서 Filter를 곱하고 Output을 만들어낸다. 이때 Receptive Field가 겹치는 영역은 값을 더해주면 된다.

그렇다면 왜 Transpose라는 것이 붙은 것일까?

![Image](https://github.com/user-attachments/assets/54a66df4-6647-4dd8-aa75-38a3d27ca032)

왼쪽은 기존의 stride 1 convolution이고 오른쪽은 stride 1 transpose convolution이다. 두 연산 모두 유사한 결과를 갖는다. 하지만 stride=2인 경우 달라진다.

![Image](https://github.com/user-attachments/assets/c8198948-020d-455b-b95c-e1b95e1d5b8b)

stride > 1인 경우에는 transpose convolution은 더이상 일반적인 convolution과 유사하게 동작하지 않는다. 

# 2. Classification + Localization

이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체가 어디에 있는지를 알고싶을수도있다.

localization 문제에서는 이미지 내에서 내가 관심 있는 객체가 오직 하나 뿐이라고 가정한다.

![Image](https://github.com/user-attachments/assets/4680940e-cf1d-4248-b3ea-20296ffa5b29)

기존의 네트워크의 형태와 비슷하지만 4개의 원소를 가진 vector와 연결된 하나의 FC-Layer가 더 추가된다. 

이런 식으로 네트워크는 Class Score와 Box coordinates 두 가지 출력값을 반환한다. 이 네트워크를 학습시킬 때는 class scores를 예측하기 위한 softmax loss, Ground truth bounding box와 예측값의 차이를 측정하는 Loss 두개 존재한다. 그러므로 학습 이미지에는 카테고리 label과 객체 bounding box ground truth를 동시에 가지고 있어야 한다.

L2 Loss로 BBox Loss를 쉽게 디자인할 수 있다.

## 2.1 Human pose estimation

![Image](https://github.com/user-attachments/assets/b5af23ab-fd98-4780-a46c-672c1bde9bd6)

이것은 사람의 이미지를 입력으로 받고 관절의 위치를 출력으로 내보내는 것이다. 대부분의 사람의 관절수는 같다는 가정을 먼저 한다.

출력으로 예측된 14개의 점에 대해서 L2 loss (혹은 regression loss)를 계산하고 backprop으로 학습한다.

Classification와 같은 categorical한 문제에서는 Cross entropy / softmax / SVM margin Loss와 같은 것들을 사용할 수 있고, Regression같은 continuous한 문제는 출력이 연속적인 값이기 때문에 L2, L1과 같은 다른 종류의 Loss를 사용한다.

# 3. Object Detection

Object detection에도 고정된 카테고리가 있다. object detection의 task는 입력 이미지가 주어지면 이미지에 나타나는 객체들의 Bbox와 해당하는 카테고리를 예측한다. 

하지만 Classification + Localization와는 조금 다르다. 왜냐하면 예측해야 하는 Bbox의 수가 입력 이미지에 따라 달라지기 때문이다. 그리고 각 이미지에 객체가 몇 개나 있을지가 미지수이다.

Object Detection이 Localization과는 다르게 객체의 수가 이미지마다 다르다. 따라서 object detection 문제로 regression 문제를 푸는 것은 어려운 문제이다.

![Image](https://github.com/user-attachments/assets/45e5f6d2-f326-4dae-b6b2-5b73bd00da8f)

## 3.1 Sliding Window

Object Detection 문제를 풀 때 많이 시도했던 방법은 sliding window이다.

앞서 semantic segmentation에서 작은 영역으로 쪼갰던 아이디어와 비슷한 방법을 사용한다.

![Image](https://github.com/user-attachments/assets/4e328496-a3d0-4af8-b13e-65dae6cfabf4)

crop된 이미지를 CNN의 입력으로 넣어 classification을 수행한다. 이때 Background라는 카테고리를 하나 추가해야한다.

이 방식의 문제는 객체의 이미지나 위치를 모르기 때문에 bruteforce한 방식으로 sliding window를 해야하고 이것은 너무 많은 경우의 수가 존재하고, 또 작은 모든 이미지를 거대한 CNN에 넣기에는 계산량이 너무 많아진다.

## 3.2 Region Proposals

![Image](https://github.com/user-attachments/assets/bf063758-e1b2-4d8a-ad07-a7c2c8ce25a7)

이 방식은 딥러닝은 아니지만 꽤 효과적일 수 있다.

이미지를 입력으로 넣고 CPU로 몇초간 계산을 돌리면 객체가 있을법한 곳의 Bbox를 제공해준다. 후보를 찾아내는 방식에는 여러가지가 있겠지만 blobby 이미지 지역을 후보로 정한다.

Region proposal을 만들어낼 수 있는 방법에는 Selective Search가 있다.

## 3.3 R-CNN

이를 통해 이미지 내의 모든 위치와 스케일을 고려하는 것이 아니라 Region Proposal Network를 적용하고 객체가 있을법한 Region Proposal을 얻어낸다. 그리고 그 이미지를 CNN의 입력으로 사용한다.

이 방식이 바로 R-CNN이다.

![Image](https://github.com/user-attachments/assets/3bc5c781-7a56-487d-9d27-0848179df905)

이미지를 입력으로 넣으면 Region of interset(ROI)를 찾아낸다. 하지만 이것은 각 ROI의 사이즈가 각양각색이라는 점이 문제가 될 수 있다. 추출된 ROI로 CNN Classification을 수행하려면 FC-Layer 등으로 인해서 보통 같은 입력사이즈로 맞춰줘야한다.

![Image](https://github.com/user-attachments/assets/8bd7d056-e045-4e34-a3e4-8703553d3bab)

Region Proposals를 추출하면 고정된 사이즈로 크기를 바꾸고 각각의 Region Proposals를 CNN에 통과시킨다. R-CNN의 경우에는 ROI들의 최종적인 classification에서 SVM을 사용했다.

또한 R-CNN은 Region Proposals를 보정하기 위한 regression 과정도 거친다. 그러므로 R-CNN은 BBox의 카테고리도 예측하지만, BBox를 보정해줄수 있는 offset 값 4개도 예측한다.

하지만 R-CNN은 여전히 문제점들이 남아있다.

- 계산비용이 높다. R-CNN은 2000개의 ROI가 있고 각각이 독립적으로 CNN의 입력으로 들어간다.
- 학습과정 자체가 오래걸린다. 디스크 공간도 많이 차지한다.
- Test time도 아주 느리다. 이미지 한 장당 대략 30초가 걸린다.

## 3.4 Fast R-CNN

Fast R-CNN은 문제점의 상당 부분을 해결했다.

![Image](https://github.com/user-attachments/assets/7196b686-1e03-452a-a5e8-99e118f7afaa)

Fast R-CNN도 R-CNN과 비슷하게 시작하지만 각 ROI마다 CNN을 적용하는것이 아니라 전체 이미지에 CNN을 적용한다. 그 결과로 전체 이미지에 대한 고해상도 Feature Map을 얻을 수 있다.

Fast R-CNN에서는 여전히 Selective Search와 같은 방법으로 Region proposals을 계산한다.

이제부터는 CNN Feature map에 ROI를 Projection 시키고 전체 이미지가 아닌 Feature map에서 가져온다. 이제는 CNN의 Feature를 여러 ROIs가 서로 공유할 수 있다.

![Image](https://github.com/user-attachments/assets/f3408c3e-82a2-4928-aa9c-801691e7e9bf)

그 다음 FC-layer가 있는데 FC-layer는 고정된 크기의 입력을 받으므로 CNN Feature map에서 가져온 ROI를 Pooling Layer를 통해 같은 크기로 조정해준다.

![Image](https://github.com/user-attachments/assets/5af3ec9c-19d7-41dc-bd64-9b5bbd43b4c0)

최종적으로 FC-layer의 입력 크기가 조정된 ROI를 넣어 Classification Score와 Linear Regression Offset을 계산할 수 있다. Fast R-CNN을 학습할 떄는 두 Loss를 합쳐 Multi-task Loss로 Backprop을 진행한다.

![Image](https://github.com/user-attachments/assets/e54046e1-3342-4708-8ba0-053c19d87304)

Fast R-CNN은 다른 방법에 비해서 훨씬 빠른 속도를 갖는다. Fast R-CNN의 Test time에서 문제가 되는 부분은 region proposal을 계산하는 구간에 정체가 생긴다는 문제점이 있다.

## 3.5 Faster R-CNN

Fast R-CNN의 문제점을 해결하는 것이 Faster R-CNN이다.

![Image](https://github.com/user-attachments/assets/daa2a76b-9030-4bac-a7cd-82b62f1fc825)

Faster R-CNN은 네트워크가 region proposal을 직접 만들 수 있다.

입력 이미지가 CNN을 거쳐 featrue map을 만들고 RPN은 네트워크가 Feature map을 사용해 Region Proposals을 계산하도록 한다.

RPN을 거쳐 Region Proposal을 예측하고 나면 나머지 동작은 fast R-CNN과 동일하게 동작한다.

Faster R-CNN은 4개의 Loss들을 한번에 학습한다. 따라서 이 Loss들의 균형을 맞추는게 까다롭다.

RPN에는 2개의 Loss가 있는데, 하나는 객체가 있는지 없는지 예측하고 나머지는 예측한 BBox에 관한 것이다.

그리고 Region Proposals의 classification을 결정하는 Loss 하나와 Region Proposal을 보정하기 위한 BBox Regression이다.

## 3.6 Detection without Proposals: YOLO / SSD

두 방법의 주요 아이디어는 각 Task를 따로 계산하지 말고 하나의 regression 문제로 풀어보자는 것이다.

![Image](https://github.com/user-attachments/assets/ab65eb30-2bbe-49a4-836c-d663463d282a)

거대한 CNN을 통과하면 모든 것을 담은 예측값이 한번에 나온다.

먼저 이미지를 7x7 같이 크게 나눈다. 각 Grid Cell 내부에는 Base BBox가 존재한다. 위 예제에서는 Base BBox가 3가지가 존재한다.

하나는 BBox의 offset을 예측한다. 실제 위치가 되려면 base BBox를 얼만큼 옮겨야 하는지

그리고 각 BBox에 대해서 Classification scores를 계산한다. 이 BBox안에 이 카테고리에 속한 객체가 존재할 가능성을 의미

출력을 보면 3차원 Tensor로 나온다. 이것을 거대한 CNN으로 한번에 학습시킨다.

Faster R-CNN은 RPN으로 먼저 Regression 문제를 풀고 ROI 단위로 Classification을 하는 방식이다. 하지만 Single Shot methods는 단 한번의 forward pass만으로 해결한다.

# 4. Instance Segmentation

Instance Segmentation은 Semantic Segmentation과 Object Detection을 합친 것이다.

예를들어 이미지 내에 두 마리의 개가 있으면, Instance segmentation은 이 두마리를 구별해야 한다. 그리고 각 픽셀이 어떤 객체에 속하는지를 전부 다 결정해야 한다.

![Image](https://github.com/user-attachments/assets/1a0cb434-20dd-4f1c-9756-2a52eb90c92f)

Mask R-CNN은 처음 입력 이미지가 CNN과 RPN을 거친다. 여기까지는 Faster R-CNN과 유사하다.

그리고 Feature map에서 RPN의 ROI만큼을 Projection 한다.

그 다음 단계는 Faster R-CNN에서 처럼 Classification/BBox Regression을 하는 것이 아니라 각 BBox마다 Segmentation mask를 예측한다. RPN으로 뽑은 ROI 영역 내에서 각각 Semantic segmentation을 수행한다.

Feature Map으로부터 ROI Pooling(Align)을 수행하면 두 갈래로 나뉜다.

첫 번째 갈래에서는 각 Region Proposal이 어떤 카테고리에 속하는지 계산한다.

두 번째는 각 픽셀마다 객체인지 아닌지를 분류한다.

![Image](https://github.com/user-attachments/assets/bb134a0e-14e9-4a26-a3cf-37567e97026f)

Mask R-CNN에서 Pose를 위한 Loss와 Layer를 하나 더 추가하면 Pose에 대해서도 동작이 가능하다.

![Image](https://github.com/user-attachments/assets/b8a96a41-43d1-4b85-8be4-388e2b584765)

![Image](https://github.com/user-attachments/assets/c99c260c-1fd3-4e46-b794-af8a37ef9032)

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
