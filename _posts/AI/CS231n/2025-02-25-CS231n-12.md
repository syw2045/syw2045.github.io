---
title: "[DL] CS231n Lecture 12: Visualizing and Understanding"

categories:
  - DL
tags:
  - [DL]

toc: true
toc_sticky: true

date: 2025-02-25
last_modified_at: 2025-02-25
---

# 1. What’s going on inside ConvNets?

Conv Net에 이미지가 입력으로 들어가고 Conv Layer를 거치면서 다양한 변환을 거친다. 그리고 출력은 이해하고 해석할 수 있는 Class score의 형태로 나온다.

그렇다면 ConvNet이 어떻게 동작하는지를 알아본다.

> First Layer

첫번째 레이어에 먼저 접근해 볼 수 있다.

첫번째 Conv Layer는 입력 이미지와 직접 내적을 수행하기 때문에 이 필터를 단순화 시키는 것만으로도 이 필터에서 어떤 이미지를 찾으려고 하는지 알 수 있다.

![Image](https://github.com/user-attachments/assets/b0095954-ab71-4833-8f5f-fa2c2d437f7b)

우선 가장 많이 찾는 것은 edges이다. 흰/검으로 길게 늘어선 필터들이 보인다.

흥미롭게도 CNN을 어떤 모델이나 데이터로 학습하더라도 첫번째 Layer는 전부 다 위와같이 생겼다. 
즉, 입력 이미지에서 oriented edges나 opposing colors를 찾는 모습을 보인다.

> Middle Layer

그렇다면 Network의 Layer 2, 3의 모습을 보면 어떻게 될까?

![Image](https://github.com/user-attachments/assets/f19cb877-f129-43a5-b859-517aad656b11)

문제점은 이 필터들을 직접 살펴보는 것 만으로는 Layer 1에서 얻은것처럼 이해할만한 정보를 얻기 힘들다. 왜냐하면 이미지와 직접적으로 연결되어 있는것이 아니기 때문이다.

시각화 한 내용은 Layer 2의 결과를 최대화 시키는 Layer 1의 출력 패턴이 무엇인지를 시각화 한것이다.

> Last Layer

CNN의 마지막 레이어에는 1000개의 클래스 스코어가 있다. 이는 학습데이터의 predicted scores를 의미한다. 그리고 이 직전 레이어에는 FC layer가 있다.

FC layer는 많은 이미지로 CNN을 돌려 나온 4096-dim의 feature vector를 모두 저장한다.

## 1.1 Visualization

### Nearest neighbors

시각화 방법중에 하나인 Nearest neighbors를 사용해 볼 수 있다.

![Image](https://github.com/user-attachments/assets/019729ec-2cc2-445b-9853-ffbf225e55d1)

feature space에서의 nearest neighbors의 결과를 보면 픽셀 값의 차이가 큰 경우도 있다. 이것은 픽셀 값의 차이는 커도 feature space내에서는 유사한 특성을 지닌다는 것을 알 수 있다.

예를들어 코끼리를 보면 첫번째는 왼쪽, 네번째는 오른쪽에 있다. pixel space에서 보면 이 둘은 아예 다른 값을 나타낼 것이다. 하지만 Network가 학습한 feature space에서는 연관된 이미지로 분류한다. 이는 네트워크가 학습을 통해 이미지의 Sementic content한 feature들을 잘 포착했기 때문이다. 

### Dimensionality Reduction

PCA는 4096-dim같은 고차원 벡터를 2-dim으로 축소하는 것이다. 이를 통해 feature space를 조금 더 직접적으로 시각화할 수 있다. PCA(principle component analysis)보다 조금 더 강력한 t-SNE(t-distributed stochastic neighbor)를 사용한다.

MNST의 각 이미지는 Gray scale 28x28이다. t-SNE는 MNST의 28x28 dim 데이터를 입력으로 받아 2-dim으로 압축한다. 그리고 압축된 2-dim을 이용해서 MNST를 시각화한다.

![Image](https://github.com/user-attachments/assets/bb264a2a-24f7-411c-ac6a-0d4b688f9116)

![Image](https://github.com/user-attachments/assets/49c7d2ec-24af-4e7c-b48f-0769cb510769)

많은 이미지를 네트워크에 통과시킨다. 그리고 각 이미지에 대해 최종 단의 4096-dim feature vector들을 저장한다. 이 벡터들에 대해 t-SNE를 적용해 2-dim으로 압축하고 2-dim feature space grid에 feature들을 시각화한다.

오른쪽 그림을 보면 좌하단에 녹색들이 모여있고, 우상단에는 하늘색들이 모여있는 것을 볼 수 있다.

## 1.2 Visualizing Activations

![Image](https://github.com/user-attachments/assets/19d45bb6-ea4d-473b-ac08-312ee2648ffb)

이 예시는 AlexNet이다. AlexNet의 conv5의 feature map은 128x13x13인데 이것을 128개의 13x13 grayscale 이미지로 볼 수 있다.

이를 시각화해보면 conv layer가 입력에서 어떤 feature를 찾고있는지 짐작해볼 수 있다. 즉, 초록색 박스처럼 네트워크의 어떤 Layer에서는 사람의 얼굴을 찾고있는지도 모른다는 것을 알 수 있다.

 

완전히 검은색인 것은 Dead ReLU가 아니라 특정 입력에 대해서 활성화 되지 않은 filter의 모습이다.

### Maximally Activating Patches

intermidate feature를 시각화할 수 있는 다른 방법은 어떤 이미지가 들어와야 각 뉴런들의 활성이 최대화되는지를 시각화해보는 방법이다.

AlexNet conv5 128x13x13 activation volume중에 한 channel(feature map)을 선택한다. 그리고 많은 이미지를 CNN에 통과시킨다. 그리고 각 이미지의 conv5 feature를 기록한다. 그리고 나서 어떤 이미지가 선택한 feature map을 최대로 활성화시키는지 본다.

![Image](https://github.com/user-attachments/assets/330e24a3-7e8e-4000-81f4-94724525f710)

각 뉴런은 이미지의 모든 부분을 보는 것이 아닌 일부분만 본다 그러므로 특정 Layer의 feature를 가장 최대화 시키는 이미지의 일부분인 patch를 구하고 시각화해 정렬한 모습이다.

이러한 patch들의 feature를 통해서 해당 뉴런이 무엇을 찾고있었는지 짐작이 가능하다.

### Occlusion Experiments

이 방법은 이미지가 CNN에 들어가기전에 일정 부분을 가리고 그 부분은 그냥 평균값으로 넣는다. 그런 다음 결과에 대해서 정확도를 평가하는 것을 반복한다. 이렇게 되면 어느 부분을 가렸을때 정확도가 낮아지는지를 알 수 있고 그 부분이 중요한 부분이라는 것을 알 수 있다.

![Image](https://github.com/user-attachments/assets/8ad53e79-484f-458f-a9ce-c2bdc03416c2)

예를들어 go-kart의 경우 앞의 go-kart를 가렸을 때 정확도가 낮아지는걸로 봐서 아마 중요한 부분이 아니었을까 하고 생각해볼 수 있다.

이러한 시각화 과정은 모델의 성능을 높이는데 도움을 주는 것이 아닌 사람이 이 모델의 동작에 대한 이해를 도와주는 과정이다.

### Saliency Maps

어떻게 픽셀들을 보고서 이미지를 개라고 분류했을지 알아보려 한다.

![Image](https://github.com/user-attachments/assets/ca1a5dcf-3619-47d3-a74a-5363c3eccf29)

이 방법은 입력 이미지의 각 픽셀들에 대해서 예측한 Class Score의 gradient를 계산하는 방법이다.

픽셀을 조금 바꾼뒤 Score를 계산하면 얼마나 변경되는지를 알 수 있고 이를 통해 어떤 픽셀이 중요한지를 파악할 수 있다.

 Saliency Map과 GrabCut를 활용하면 supervised 없이도 segmentation을 할 수 있다.

### guided backpropagation

이미지의 각 픽셀에 대한 Class Score의 gradient를 계산하는 것이 아니라 입력 이미지의 각 픽셀에 대한 네트워크 중간 뉴런의 gradient를 계산한다. 이를 통해 어떤 픽셀이 해당 뉴런에 영향을 주는지 알 수 있다.

이러한 backpropagation 과정에서 약간의 trick을 더하면 더 깨끗한 이미지를 얻을 수 있다. 이 방법을 guided backpropagation이라고 하고 backprop 시에 ReLU를 통과할 때 약간의 변형을 준다.

![Image](https://github.com/user-attachments/assets/d3097051-c97f-4be7-a6bb-7fe6b5770fab)

guided backpropagation은 gradient가 0보다 크면서, ReLU의 output이 양수인 부분만 backprop해준다.

![Image](https://github.com/user-attachments/assets/5e7aa418-84b5-441c-b46e-732c8406e8a3)

이를 통해 네트워크 중간에서 어떤 것을 찾고있었는지 이해해볼 수 있다.

guided backpropagation이나 saliency map을 계산하는 방법들은 고정된 입력 이미지 혹은 입력 patch의 어느 부분이 뉴런에 영향을 미치는지를 알려준다.


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
