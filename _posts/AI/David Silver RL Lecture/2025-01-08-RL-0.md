---
title: "[RL] David Silver RL Lecture 1: Introduction to Reinforcement Learning"

categories:
  - RL Lecture
tags:
  - [RL]

toc: true
toc_sticky: true

date: 2025-01-04
last_modified_at: 2025-01-10
---
# 1. Abot RL

## 1.1 Branches of Machine Learning
![image](https://github.com/user-attachments/assets/ddfb9d33-339d-4f2d-8213-188af9108835)

Machine Learning 안에는   

- Supervised Learning (지도 학습)
- Unsupervised Learning (비지도 학습)
- Reinforcement Learning (강화 학습) 이 있다.

## 1.2 Characteristics of Reinforcement Learning
그렇다면 강화 학습은 다른 머신 러닝 패러다임과 어떤 것이 다른 걸까?

- Supervisor가 없고, reward만 존재
    - 강화 학습은 “정답”을 알려주는 사람(Supervisor) 없이 Agent가 Reward만 받으면서 최적의 길을 찾는다.
    - Q) reward랑 “정답”이랑 어떤 차이점?
    - A) 뭘 해야 reward를 받는지는 안 알려주고 알아서 reward를 maximize한다.
- Feedback(reward)이 즉각적이지 않다.
    - 예를 들어 어떤 것에 대한 보상을 10분 후에 알려줌
- Time이 정말 중요하다. (순서가 있고, non i.i.d data)
    - independant and identically distribution → 각각의 사건이 다른 사건에 영향을 주지 X
    - 어떤 행동의 순서도 중요하다(왼쪽 → 오른쪽 ≠  오른쪽 → 왼쪽)
- Agent의 Action이 받는 데이터에 영향을 미친다.
    - Action에 따라 다음에 받는 데이터가 달라지고 어떤 데이터를 어떻게 받을 지에 대한 고려도 필요하다.

## 1.3 Examples of Reinforcement Learning
강화학습을 통해 다양한 것들을 할 수 있다. (ex 휴머노이드 로봇 걷게 하거나, 아타리 게임을 사람보다 잘하거나 등등)

# 2. The RL Problem

## 2.1 Rewards
> Reward   

- reward  $$R_t$$ 는 scalar feedback signal이다.
    - reward를 scalar 값으로만 표현하기 힘든 문제는 RL로 풀기 어렵다
- step t 일 때 agent가 얼마나 잘 하는지 나타내는 지표
- agent의 목적은 축적된 reward를 maximize 하는 것


> Definition (Reward Hypothesis)   

모든 Goals은 “<u>기대되는 축적된 보상의 최대화</u>"로 설명될 수 있다.

> Sequential Decision Making

- Goal : 미래 <u>보상의 총합을 최대화</u>하는 Action을 선택
- Action은 장기적인 결과를 가져올 수 있음
- reward는 지연될 수 있음
- <u>미래의 더 큰 보상을 위해서 즉각적인 보상을 포기</u>하는 것이 더 나을 수 있다.

## 2.2 Agent and Environment

![image 2](https://github.com/user-attachments/assets/d559adbb-17d4-4afb-9e32-4a7d1866a84a)   

> Agent ↔ Enviroment : 상호작용
- Environment : Agent가 Action을 하면 Environment에서 반응을 해 Reward와 Observation을 준 다.
- Agent : Observation과 Reward를 받고 다음 Action을 한다.

    

## 2.3 History and State

> History

History : 시간 t까지 있었던 모든 각각의 timestep마다 Agent가 수행한 action과 그때의 observation, reward를 순차적으로 기록한 것

![image 3](https://github.com/user-attachments/assets/bfc4e531-a031-48d6-b750-96be230de2ae)

State 는 다음 행동을 결정하는 정보, State는 History의 함수

![image 4](https://github.com/user-attachments/assets/4fec29e6-ab16-47c2-8392-2bc79daf5647)

> Environment State

![image 5](https://github.com/user-attachments/assets/940d9a3f-b15b-4416-878c-1e10e953fa38)

- Environment가 Action 후 Observation, Reward를 계산하는 데 쓴 정보들
- Agent한테 일반적으로 보이지는 않지만, environment state가 보인다 하더라도 너무 쓸모없는 정보가 많을 수도 있다.

> Agent State

![image 6](https://github.com/user-attachments/assets/d920e798-678a-41ef-925c-fe1f171c2a57)

- <u>다음 Action을 선택하기 위해 참조하는 정보들</u>, State는 History의 함수
- ex) A주식을 산다 할 때, 거래량, 재무제표를 보겠다.
    - “거래량”, “재무제표” → Agent의 State

> Information State(a.k.a Markov state)

- History로 부터 모든 정보들을 포함하는 State
- Markov State : “현재가 주어져 있다면, 과거와는 독립적이다.”

다음 State를 결정 할 때, 그 이전 모든 State는 필요 없고, 바로 현재 t의 State 만 본다

![image 7](https://github.com/user-attachments/assets/7b0faed1-e050-4a84-88f9-bb8c375a39d1)

$$S_t$$가 일어났을 때 $$S_{t+1}$$의 확률 = $$S_1,...,S_t$$가 일어났을 때 $$S_{t+1}$$의 확률   
→ 다음 State의 확률은 현재 State만 영향을 준다.   

- 한번 State를 알면, History는 필요가 없다
    - State는 미래에 대한 충분한 통계치이다. (미래를 파악할 수 있다.)
- Environment State는 Markov State일 수밖에 없다.
- History도 Markov하다.

Markov State ex) 현재 State에 내 차의 속도를 포함하여 정해 놓은 상태에서, 속도를 추측
- 바로 직전의 속도를 알고 있다. → 현재 속도 추측 가능


Not Markov State ex) 현재 State에 내 차의 속도를 모르는 상태에서 엑셀을 밞았을 때, 그 전 state의 다른 매개변수를 확인해야 지만 속도를 추측 가능한 경우

- 아무런 정보를 모른 상태에서 엑셀을 밟음 → 속도 추측을 하기 위해선
    - 직전의 속도, 직전 차의 위치 등등을 알아야 추측 가능

> Rat Example

![image 8](https://github.com/user-attachments/assets/97f6c683-b169-468e-9c53-9e992eab950f)

![image 9](https://github.com/user-attachments/assets/1c9a9449-1afe-4592-9245-a4336d631f8c)

전구 → 레버 → 종 = 전기

전구 → 레버 → 레버 = 치즈

전구 → 레버 → 종 = ?  ⇒ 전기

![image 10](https://github.com/user-attachments/assets/c77f43de-4ee1-44bd-beec-ddff12115101)

전구 2, 레버 1, 종 1 = 전기

전구 1, 레버 2, 종 1 = 치즈

전구 1, 레버 2, 종 1 = ?  ⇒ 치즈

![image 11](https://github.com/user-attachments/assets/ba41708e-2891-42cf-bb47-3fb79e6e53a3)

전구 → 전구 → 레버 → 종 = 전기

종 → 전구 → 레버 → 레버 = 치즈

레버 → 전구 → 레버 → 종 = ?  ⇒ 알수없음

⇒ <u>따라서 State를 어떻게 설정하느냐에 따라 같은 데이터라도 결과가 달라질 수 있다.</u>

## 2.4 Environments
> Fully Observable Environments

- Full observability: agent가 environment State를 모두 관찰 가능한 경우   
Agent St = env St = information St

- 이와 같은 경우를 형식적으로 **Markov decision process(MDP)**라고 한다.

> Partially Observable Environments

- Partial observability : agent가 environment State를 관찰이 불가능 한 경우
Agent State ≠ environment State
    - ex)포커 시에 상대 패를 알 수 없는 경우, 로봇의 카메라가 정확한 위치를 알려주지 않는 경우
- 형식적으로는 Partially Observable MDP (POMDP)라고 한다.
- agent가 state를 표현하는 방식이 여러 개 존재. history, RNN 등등

# 3. Inside An RL Agent
## 3.1 Major Components of an RL Agent

- RL agent는 1개 이상의 components를 포함한다.
    - policy
    - Value function
    - Model

> Policy

- Policy : Agent의 행동을 규정
- State를 넣으면 Action이 나옴

![image 12](https://github.com/user-attachments/assets/7c08078b-72f9-41a2-b073-8ef88bf0884d)

- Deterministic policy : State 대입 시 action을 결정적으로 선택 (정해져 있다)
- Stochastic policy : State 대입 시 여러 action별로 확률을 고려하여 선택

> Value Function

- Value Function : 미래 reward의 총 합산을 예측
- 현재 State가 좋은지 나쁜지 평가하는데 쓰임

![image 13](https://github.com/user-attachments/assets/88def14a-754c-4cd4-aed1-938e59fcfc65)

State 대입 시에 어떤 Policy를 따라갔을 때의 얻을 총 reward의 기대값 =
reward들의 기대값, t가 증가할 수록 discount vector를 적용하여 현재 혹은 미래에 대한 가중치 또한 고려

⇒ 기대값인 이유

- Stochastic policy인 경우 확률에 따라 결과값이 다를 수 있음
- Deterministic policy의 경우에도 환경 자체도 확률적 요소가 있기 때문에 같은 action 진행 시에도 결과값이 달라질 수 있다. 이것을 모두 고려해 기대값 측정

- <u>Policy가 없으면 Value function도 정의가 불가능</u>

> Model

- Model : environment가 어떻게 될지 다음을 예측하는 것, RL에서는 쓰일 수도 안쓰일 수도 있다.
    - Model-Free - Model-based
    - reward 혹은 State transition을 예측하는 두가지 방식이 존재

## 3.2 Maze Example

> Policy

![image 14](https://github.com/user-attachments/assets/8ffe3ded-2c92-450f-8cba-01023bb2cbf7)

![image 15](https://github.com/user-attachments/assets/a3f2e245-87d8-4b45-8b1b-e0fee33516cf) 
- optimal policy

> Value Function

![image 16](https://github.com/user-attachments/assets/25f08f64-1e59-453d-8817-bdcd4af17ec2)

- Optimal Policy를 따랐을 때의 Value Function

> Model

![image 17](https://github.com/user-attachments/assets/02029639-08ce-4b39-97c6-b22b37d423b8)


- Agent는 내부적으로 Environment에 대한 model을 보유함
    - 위 Grid는 실제 Environment가 아니라, Agent가 생각하는 model → (imperfect)
    - Model은 reward와 state transition을 예측해야 한다.
- Model은 imperfect할 수 있다
- Dynamics: action에 따라 state가 어떻게 바뀌는지
- Rewards: 각 state 마다의 reward

![image 18](https://github.com/user-attachments/assets/8e3d0520-f513-4111-8fb8-e422d7761ef4)

- $$P^a_{ss'}$$ :  s에서 action을 하면 s’으로 갈 확률
- $$R^a_s$$ : 각 state s 에서 a를 하면 즉시 받을 reward

## 3.3 Categorizing RL agents

![image 19](https://github.com/user-attachments/assets/0b5b0948-1ae8-42f3-bbf7-cb95fc0e59dc)

- Value Based → 그냥 Value가 높은 곳으로 가
    - No ~~Policy (Implicit)~~
    - Value Function 만 있음

- Policy Based → 그냥 앞으로만 가
    - Policy만 있음
    - No ~~Value Function~~

- Actor Critic
    - Policy
    - Value Function

---

- Model Free → model을 만들지 않고, Policy + Value function으로만 구성되는 agent
    - Policy and/or Value Function
    - No ~~Model~~

- Model Based → 내부적으로 environment model을 추측하여 환경을 추측하여 동작
    - Policy and/or Value Function
    - Model

# 4. problems within RL

## 4.1 Learning and Planning

- Reinforcement Learning
    - 환경을 모름 (reward, state transition을 모른다)
    - agent는 환경과 상호작용한다.
    - agent는 policy를 개선해나간다.

- Planning
    - 환경의 model을 안다 (reward, state transition)
    - agent는 실제 상호작용 없이도 모델을 알기 때문에 computation을 통해 수행이 가능하다
    - agent는 policy를 개선해나간다. ex) Monte-carlo

⇒ 모델을 아느냐 모르냐에 따라 Policy 개선 방식에 차이가 존재

## 4.2 Exploration and Exploitation

- RL은 환경과 상호작용하면서 겪었던 경험들을 통해 좋은 Policy를 찾는 Learning 과정

- Exploration : 환경으로부터 정보를 얻어 환경을 이해하는 과정 ( 새로운 것을 시도 )
- Exploitation : 얻은 정보를 바탕으로 reward를 maximise하는 과정 (아는 것 중에 가장 좋은 것)

⇒ 두 가지 방법 모두 중요,
      Exploration을 통한 정보 수집, 그에 대한 Exploitation으로 이루어진 trade-off 관계

- EX) 음식점 선택시
    - Exploration : 새로운 맛있는 음식점을 찾으려 새로운 곳을 시도
    - Exploitation : 내가 아는 음식점 중 가장 맛있는 곳으로 감

## 4.3 Prediction and Control

- Prediction : 미래를 평가, <u>Value function을 잘 학습시키는 것</u>
    
   ![image 20](https://github.com/user-attachments/assets/ff5b2a68-87f2-4c66-9970-01445494d44e)
    
    Grid world에서 random uniform policy로 무한히 선택했을 때 state 마다의 value function(future reward)
    
    어떻게 찾는 지는 중요하지 않고, 학습을 통한 value 값을 찾는 문제
    
- Control : 미래를 최적화, <u>Best Policy를 찾는 것</u>
    
    ![image 21](https://github.com/user-attachments/assets/8fede3bc-16b1-4063-9d78-2021c8e59833)
    

    state에서 어떻게 움직여야 하는 지를 찾는 문제, <u>optimal policy를 찾는 것</u>

    $$v_*$$는 optimal한 value funcion, $$π_*$$는 optimal한 policy를 의미   

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}