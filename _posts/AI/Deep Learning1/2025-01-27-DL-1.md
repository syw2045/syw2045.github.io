---
title: "[DL] Chapter 3: 신경망 (Neural Network)"

categories:
  - Deep Learning
tags:
  - [AI]

toc: true
toc_sticky: true

date: 2025-01-27
last_modified_at: 2025-01-27
---

밑바닥부터 시작하는 딥러닝1를 읽고 정리한 내용입니다✏️.
{: .notice--warning}

퍼셉트론으로 복잡한 함수도 표현이 가능하다는 것을 알았다. 하지만 여전히 가중치를 설정하는 작업은 사람이 수동으로 한다는 문제가 있었다.

하지만 신경망은 이 문제점을 해결할 수 있다.

<u>가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력</u>이 신경망의 중요한 성질이다.

# 1. 신경망

![Image](https://github.com/user-attachments/assets/dfe0e929-28fd-4d89-b509-c3a3f0424f57)

신경망은 입력층, 은닉층, 출력층으로 나타낼 수 있다.

은닉층은 사람 눈에는 보이지 않는다.

편향까지 추가된 퍼셉트론의 수식을 보면 다음과같다.

![Image](https://github.com/user-attachments/assets/a882d279-4d24-43ed-b2d1-a1250a3570ff)

편향(bias) : 뉴런이 얼마나 쉽게 활성화되느냐를 제어

가중치(Weight) : 각 신호의 영향력 제어

위 수식을 더 간단히 나타내면 아래와 같다.

![Image](https://github.com/user-attachments/assets/fe49f2f5-ccfd-4d7a-875c-ab4d9774d7ad)

입력신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 그 값이 y의 출력이 된다.

## 1.1 활성화 함수(Activation function)

입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 활성화 함수(activation function)이라고 한다.

<u>활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할</u>을 한다.

다음과 같이 그림으로 처리 과정을 나타낼 수 있다.

![Image](https://github.com/user-attachments/assets/69f53d23-952e-4353-8da9-74fe54208024)



퍼셉트론은 활성화 함수로 쓸 수 있는 여러 후보 중에서 계단 함수를 채용하고 있다.

신경망에서 자주 이용하는 활성화 함수는 시그모이드 함수(sigmoid function)이다.

![Image](https://github.com/user-attachments/assets/94f9a891-d032-41f4-92e6-bf8c29db4e81)

![Image](https://github.com/user-attachments/assets/ee44a54d-9eb7-4098-9a25-2320ea772d22)

> 차이점

계단 함수가 0과 1중 하나의 값만 돌려주는 반면 시그모이드 함수는 실수를 돌려준다는 점이 다르다.

즉 퍼셉트론에서는 뉴런 사이에 0 혹은 1이 흘렀다면, 신경망에서는 연속적인 실수가 흐른다.

> 공통점

둘 다 입력이 커지면 출력이 1에 가까워지고, 입력이 작을 때의 출력은 0에 가깝다.

즉 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다. 

입력이 아무리 작거나 커도 출력은 0에서 1사이의 값이다.

둘 모두 비선형 함수이다.

## 1.2 비선형 함수

무언가 입력했을 때 출력이 입력의 상수배만큼 변하는 선형 함수가 아닌, 즉 직선 1개로는 그릴 수 없는 함수이다.

신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.

→ <u>선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다.</u>

선형 함수 h(x) = cx를 활성화 함수로 사용한 3층 네트워크가 있다고 하면

y(x) = h(h(h(x)))가 된다. 이 계산은 y(x) = c * c * c * x처럼 곱셈을 세번 수행하지만, 실은 y(x) = ax와 같은 식이다.
a = $$c^3$$이라고 하면 끝이다. 즉 은닉층이 없는 네트워크로 표현할 수 있다.

선형 함수를 이용해서는 여러 층으로 구성하는 이점을 살릴 수 없다. 

## 1.3 ReLU 함수 (Rectified Linear Unit)

입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0을 출력하는 함수

![Image](https://github.com/user-attachments/assets/b109afb9-199d-4f06-949d-0e37a826e04b)

![Image](https://github.com/user-attachments/assets/7fe3cf95-f31f-4844-aef7-41fc5edbeedf)

# 2. 출력층 설계하기

신경망은 분류와 회귀 모두에 이용할 수 있다. 다만 둥 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라진다.   
일반적으로 회귀에는 항등 함수를, 분류에는 소프트맥스 함수를 사용한다.

## 2.1 항등 함수 (identify function)

입력과 출력이 항상 같다

출력층에서 항등 함수를 사용하면 입력 신호가 그대로 출력 신호가 된다.

![Image](https://github.com/user-attachments/assets/2821fd50-f74b-434d-9a5f-c0b39b76f0dc)

## 2.2 소프트맥스 함수(Softmax function)

소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받는다. 분모를 보면 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문이다.

![Image](https://github.com/user-attachments/assets/8a28af79-d90c-4b3c-98e6-0f537b111a91)

![Image](https://github.com/user-attachments/assets/51a6712e-a3bf-4b3a-ad8e-abe34c08b367)

위의 식은 컴퓨터로 계산할 때 오버플로가 생길 수 있다는 문제가 있다.   
컴퓨터는 수를 유한한 데이터로 다루기 때문에 일정 크기가 넘어가면 $$\inf$$가 되어 돌아온다. 이런 큰 값끼리 나눗셈을 하면 수치가 불안정해진다.

즉 소프트맥스 함수 수식을 개선할 수 있다.

![Image](https://github.com/user-attachments/assets/889ddf65-001a-4c35-8a8f-61e055aff114)

위에서 보듯 소프트맥스이 지수 함수를 계산할 때 어떤 정수를 더하거나 빼도 결과는 바뀌지 않는다.

일반적으로 입력신호의 최대값을 $$C'$$으로 사용한다.

결과를 보면

소프트맥스의 함수의 출력은 0에서 1.0 사이의 실수이다. 출력의 총합은 1이다.

<u>출력은 총합이 1이라는 성질을 통해 출력을 **확률**로 해석할 수 있다.</u>

> 주의점

소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다.(y = exp(x)가 단조 증가 함수이기 때문)

즉 a의 원소들 사이의 대소 관계가 y의 원소들 사이의 대소 관계로 그대로 이어진다.

⇒ 신경망으로 분류를 할 때는 출력층의 소프트맥스 함수를 생략해도 된다. (계산에 드는 자원 낭비 줄일 수 있음)

## 2.3 출력층의 뉴런 수

분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적

예를 들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 출력층의 뉴런 10개로 설정

![Image](https://github.com/user-attachments/assets/251f30c8-178d-4251-95e1-0bde15d61f3e)

## 2.4 신경망의 추론 처리

MNIST 데이터 셋을 가지고 추론을 수행하는 신경망을 구현하다고 해보자

입력층 뉴런을 784개, 출력층 뉴런을 10개로 구성한다.

이유는 이미지 크기가 28x28=784 이고 0부터 9까지의 숫자를 구분하는 문제이기 때문이다.

은닉층은 총 두 개로, 첫 번째 은닉층에는 50개의 뉴런, 두 번째 은닉층에는 100개의 뉴런을 배치(임의의 값)

![Image](https://github.com/user-attachments/assets/c1fb62c2-5a20-47fc-9fca-30ba0badd65b)

다음과 같은 과정을 거쳐 입력층 784에서 출력층 10으로 나온다.

이러한 과정을 100장의 입력 데이터를 동시에 진행하면 다음과 같이 계산되고

이처럼 하나로 묶은 입력 데이터를 배치(Batch)라고 한다.

![Image](https://github.com/user-attachments/assets/79cc4956-444e-416a-be27-7c0dec28f4db)

배치 처리는 컴퓨터로 계산할 때 이미지 1장당 처리 시간을 대폭 줄여준다. 그 이유는 

1. 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도로 최적화되어 있음
2. 버스에 주는 부하를 줄인다. (느린 I/O를 통해 데이터를 읽는 횟수가 줄어, 빠른 CPU나 GPU로 순수 계산을 수행하는 비율이 높아진다.)

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
