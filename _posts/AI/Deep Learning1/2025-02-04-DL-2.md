---
title: "[DL] Chapter 4: 신경망 학습"

categories:
  - Deep Learning
tags:
  - [AI]

toc: true
toc_sticky: true

date: 2025-02-04
last_modified_at: 2025-02-04
---

밑바닥부터 시작하는 딥러닝1를 읽고 정리한 내용입니다✏️.
{: .notice--warning}

# 4. 신경망 학습

학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것
손실 함수 : 신경망이 학습을 할 수 있도록 해주는 지표
학습의 목표 : <u>손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것</u>

## 4.1 데이터에서 학습한다!

신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.

즉, 데이터에서 "학습"한다는 것은  데이터를 통해 <u>가중치 매개변수의 값을 자동으로 결정한다.</u> 

### 4.1.1 데이터 주도 학습

기계학습의 중심에는 데이터가 있다.   
기계학습은 데이터를 통해 답을 찾고 패턴을 발견하는 과정이다. 

어떤 문제를 해결하려 할 때 특히 패턴을 찾아내야 하는 경우    
사람은 직접 여러 가지를 생각을 통해 답을 찾지만  
기계학습은 사람의 개입을 최소화하고 수집한 데이터에서 패턴을 학습한다.

신경망과 딥러닝은 기존 기계학습 방법보다 사람의 개입을 더욱 배제할 수 있도록 도와준다.  
예를 들어 이미지에서 숫자를 인식하는 프로그램을 만든다고 가정해보자.  
이 과정은 일반적으로 다음과 같이 진행된다. 

1. Feature 추출: 이미지에서 중요한 특성을 사람이 설계하여 추출한다.  
2. 기계학습 적용: 추출한 feature를 바탕으로 패턴을 학습한다.  

여기서 feature란 입력 데이터(예: 이미지)에서 본질적인 정보를 추출할 수 있도록 설계된 변환기이다.  
이미지의 feature는 보통 벡터(vector)로 표현된다.  

기계학습에서는 데이터를 분석하고 규칙을 찾아내는 역할을 기계가 담당하지만 
이미지를 벡터로 변환하는 과정에서 사용하는 feature는 여전히 사람이 설계해야 한다.  
즉, 문제에 적합한 feature를 사용하지 않으면 좋은 결과를 얻기 어렵다. 

![Image](https://github.com/user-attachments/assets/a7dcd7e3-e840-420c-a6b2-dd02609a4f03)

제일 하단의 경우 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는 것을 end-to-end machine learning이라고 한다.

신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다.

‘5’를 인식하는 문제든 ‘개’를 인식하는 문제든 신경망은 주어진 데이터를 온전히 학습하고 주어진 문제의 패턴을 발견하려 시도한다.

즉, 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 ‘end-to-end’로 학습할수있다.

### 4.1.2 훈련 데이터와 시험 데이터

앞서 데이터가 중요하다고 언급했듯이 기계학습 문제에서는 데이터를 훈련 데이터(Training data)와 시험 데이터(Test data)로 나누어 훈련(Training)과 평가(Test)를 수행한다.  

이렇게 데이터를 나누는 이유는 범용적으로 사용할 수 있는 모델을 만들기 위해서다.  
범용 능력이란 아직 보지 못한 데이터에도 올바르게 대응하는 능력을 의미한다.  
이를 평가하기 위해 데이터를 두 개로 나누어 검증하는 것이다.  

만약 하나의 데이터셋만으로 학습과 평가를 모두 수행하면 학습한 데이터에서는 높은 성능을 보이지만 새로운 데이터에서는 성능이 저하될 위험이 있다.  
즉, 특정 데이터에 지나치게 최적화된 모델이 되어 범용성이 떨어지는 문제(overfitting)가 발생할 수 있다.  

## 4.2 손실 함수

신경망 학습에서는 현재 모델의 성능을 나타내는 지표가 필요하다.  
이때 사용되는 것이 손실 함수(Loss function)이고 <u>모델이 학습을 통해 손실 함수를 최소화하도록 가중치 매개변수를 최적화하는 것이 목표</u>다.  

손실 함수는 신경망 성능의 ‘나쁨’을 나타내는 지표로 일반적으로 다음과 같은 두 가지 함수가 많이 사용된다.  

- 오차제곱합(Sum of Squared Errors, SSE)  
- 교차 엔트로피 오차(Cross-Entropy Error, CEE) 

손실 함수를 줄이는 방향으로 학습이 진행되고 이를 통해 신경망 모델이 점점 더 성능이 향상되도록 한다.

### 4.2.1 오차제곱합

![Image](https://github.com/user-attachments/assets/16d4e36b-0747-4505-a910-c076639276d0)

가장 많이 쓰이는 손실 함수는 오차제곱합(Sum of squares for error, SSE)이다.

### 4.2.2 교차 엔트로피 오차

![Image](https://github.com/user-attachments/assets/93c4f2a8-ad4e-42d4-a1b9-926adf97e0ca)

또 다른 손실 함수로서 교차 엔트로피 오차(Cross entropy error, CEE)도 자주 사용한다.

$$y_k$$ : 신경망의 출력(0 ~ 1)   
$$t_k$$ : 정답 레이블 (정답이면 1, 그외 0)

교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.  
정답에 해당하는 출력이 커질수록 0에 다가가다가 그 출력이 1일 때 0이 된다. 반대로 정답일 때의 출력이 작아질수록 오차는 커진다.

### 4.2.3 미니배치 학습

기계학습 문제는 훈련 데이터를 사용해 학습한다.

훈련 데이터에 대한 손실 함수의 값을 구하고 그 값을 최대한 줄여주는 매개변수를 찾는다.

이를 위해서는 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다.

교차 엔트로피 오차로 예를 들어 평균 손실 함수를 구하면 아래와 같다.

![Image](https://github.com/user-attachments/assets/6e485783-1d4e-4551-b3e6-b19ac89e71f1)

정확한 모델을 위해서 모든 데이터에 대해 손실 함수를 구하면 좋겠지만 데이터가 너무 많으면 현실적으로 불가하다. 이런 경우 데이터 일부를 추려 전체의 ‘근사치’로 이용할 수 있다.

신경망 학습에서도 훈련 데이터로부터 ‘일부’만 골라 학습을 수행할 수 있고 이것을 미니배치 학습이라고 한다. (일부 : 미니 배치(mini batch))

### 4.2.5 왜 손실 함수를 설정하는가?

지금까지 내용을 정리해보면   
신경망은 훈련 데이터(Training data)와 시험 데이터(Test data)를 이용해 모델을 학습하고 평가한다.  

모델의 성능을 평가하는 지표 중 하나로 손실 함수(Loss function)를 사용하고
일반적으로 오차제곱합 또는 교차 엔트로피 오차가 활용된다.  

손실 함수를 계산하면 평균 손실 함수(Mean Loss Function)를 구할수 있고 <u>이 값을 최소화하는 최적의 매개변수(가중치와 편향)를 찾는 것이 신경망 학습의 목표</u>이다.  

> 정확도를 지표로 사용할 수도 있는데 왜 손실 함수를 사용할까?  

이 질문의 답은 신경망 학습에서의 ‘미분’의 역할을 보면 알 수 있다.  

신경망 학습에서는 손실 함수의 값을 최소화하는 매개변수 값을 찾기 위해 미분 값을 단서로 매개변수를 갱신하는 과정을 반복한다.  
즉, <u>매개변수를 아주 조금 변화시켰을 때 손실 함수가 어떻게 변하는지(미분 값)를 계산하고 이를 기반으로 최적화가 이루어진다.</u>

그러나 정확도를 지표로 삼을 경우 문제가 발생한다.  

> 정확도를 지표로 하면 매개변수를 갱신할 수 없는 이유  

정확도를 지표로 하면 미분 값이 대부분의 장소에서 0이 되어버린다. 왜냐하면 정확도가 연속적인 값이 아니라 불연속적인 값이기 때문이다.  

예를 들어
- 100장의 데이터 중 32장을 올바르게 인식했다면 정확도는 32%이다.  
- 매개변수를 약간 조정해도 정확도는 그대로 유지되고 변화하더라도 32% → 33% → 34%처럼 불연속적으로 증가한다.  
- 이런 불연속적인 변화는 미분을 적용할 수 없게 만든다.  

따라서 미분을 이용한 최적화가 가능한 손실 함수를 사용해야 한다.  
손실 함수는 연속적인 값으로 표현되므로 매개변수의 변화에 따라 미세한 조정을 할 수 있다.  
이렇게 <u>신경망 학습에서는 손실 함수를 통해 매개변수를 서서히 갱신하고 최적화하는 과정</u>이 이루어진다.

## 4.3 기울기

기울기(Gradient)란 모든 변수의 편미분을 벡터로 정리한 것이다.  
기울기는 각 지점에서 함수의 출력 값을 가장 크게 줄이는 방향을 가리킨다.  
즉, <u>기울기가 가리키는 방향으로 이동하면 함수의 값이 감소한다.</u>

### 4.3.1 경사법(경사 하강법 Gradient Descent)

기계학습 문제의 대부분은 학습 단계에서 최적의 매개변수를 찾아야 한다.  
신경망 또한 최적의 가중치와 편향을 학습 과정에서 찾아야 하고 이때 <u>손실 함수가 최소가 되는 매개변수 값을 찾는 것</u>이 목표이다.  

각 지점에서 함수의 값을 줄이는 방향을 제시하는 지표가 기울기(Gradient)이지만 기울기가 가리키는 방향에 반드시 함수의 최솟값이 있는 것은 보장되지 않는다.  

즉,신경망 학습에서는 손실 함수를 최소화해야 하고 기울기는 손실 함수를 최소화할 가능성이 높은 방향을 제시하는 역할을 한다.  

경사법(Gradient Method)은 현재 위치에서 기울어진 방향으로 일정 거리만큼 이동하고 이렇게 함수의 값을 점진적으로 줄여나가는 방법이다.  

- 학습률(Learning Rate): 매개변수 값을 얼마나 갱신할지 결정하는 값  
- 하이퍼파라미터(Hyperparameter): 사람이 직접 설정해야 하는 매개변수 (예: 학습률)  
- 파라미터(Parameter): 훈련 데이터와 학습 알고리즘을 통해 자동으로 학습되는 매개변수  

## 4.4 학습 알고리즘 구하기

신경망의 학습 절차는 다음과 같이 진행된다.  

> 전제  

신경망에는 조정 가능한 가중치와 편향이 있으고 이들을 훈련 데이터에 맞게 조정하는 과정이 학습(Learning)이다.  

> 1단계 - 미니배치(Mini-Batch)  

훈련 데이터 중 일부를 무작위로 샘플링한다.  
이렇게 선택된 데이터 집합을 미니배치(Mini-Batch)라고 하고 미니배치의 손실 함수 값을 줄이는 것이 목표이다.  

> 2단계 - 기울기 산출(Gradient Calculation)  

각 가중치 매개변수에 대한 기울기(Gradient)를 구한다.  

> 3단계 - 매개변수 갱신(Parameter Update)  

가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.  

> 4단계 - 반복  

위 과정을 반복하여 손실 함수 값을 점점 줄인다.  
이 방법을 경사 하강법(Gradient Descent)이라고 한다.   
특히 미니배치를 무작위로 선정하는 방식이므로 이를 확률적 경사 하강법(Stochastic Gradient Descent, SGD)라고 한다.


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
