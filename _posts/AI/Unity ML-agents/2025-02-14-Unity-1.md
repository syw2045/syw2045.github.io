---
title: "[RL] Unity ML-agents Chpater 1 : 강화학습의 개요"

categories:
  - RL
tags:
  - [RL, Unity]

toc: true
toc_sticky: true

date: 2025-02-14
last_modified_at: 2025-02-14
---

파이토치와 유니티 ML-Agents로 배우는 강화학습을 읽고 정리한 내용입니다✏️.
{: .notice--warning}

David Silver 교수님 강의를 통해 개념들을 이해했지만, 한번 더 정리하면서 복습하는 것으로 생각하면 좋을 것 같다.

# 1. 강화학습의 개요

## 1.1 강화학습이란?

기계학습 : 기계가 <u>일일이 코드로 명시하지 않은 동작</u>을 데이터로부터 학습하여 실행할 수 있도록 하는 알고리즘을 개발하는 연구 분야

일반적으로 프로그래밍이란 구현하고자 하는 기능을 명확한 코드로 작성하는 것이다. 하지만 기계학습은 이와 다르게 기능에 대한 명확한 코드를 입력하지 않고, 데이터로부터 학습한 대로 판단하게 한다. (Ex. Decision tree, Random forest, SVM, ANN)

기계학습은 크게 지도학습, 비지도학습, 강화학습으로 나눌 수 있다.

그중 강화학습은 <u>지도 학습처럼 정답이 있지도 않고</u>, 비지도 학습처럼 <u>데이터의 특징만을 기반으로 학습하지도 않는다.</u> 강화학습은 <u>에이전트가 환경과 상호작용</u>하고, 이 <u>환경에는 보상이라는 기준</u>이 있어서 다양한 시행착오를 겪어가며 <u>보상을 최대화하는 방향으로 학습</u>한다.

---

## 1.2 강화학습의 기초 용어

- 에이전트(Agent) : 강화학습에서 의사결정을 하는 대상.
    - 게임에서 제어의 대상, 캐릭터
    
- 환경(Environment) : 에이전트의 의사 결정을 반영하고 에이전트에게 정보를 주는 역할.
    - 게임 화면, 점수, 목숨 정보와 같은 것을 바꿔주는 게임 시스템 그 자체

- 관측(Observation) : 환경에서 제공해주는 정보
    - 유니티 ML-agents에서는 Visual observation과 Vector observation으로 나눈다.

- 상태(State) : 에이전트가 의사 결정하는 데 사용하기 위해 관측, 행동 등을 가공한 정보
    - 에이전트는 상태를 기반으로 의사 결정을 한다.

- 행동(Action) : 에이전트가 의사 결정을 통해 취할 수 있는 행동
    - 행동에는 Discrete action과 Continuous action이 있다.

- 스텝(Step) : 특정 상태에서 한번 행동을 취하여 다음 상태가 되는 경우
- 에피소드(Episode) : 에이전트가 1 스텝씩 계속 행동해서 게임 한판이 종료되는 경우
- 상태 변환 확률(State transition probability) : 상태 s에서 행동 a를 했을 때 다음 상태 s’이 될 확률
- 정책(Policy) : 특정 상태에서 취할 수 있는 행동을 선택할 확률 분포

---

앞서 강화학습은 시행착오를 겪으면서 <u>보상을 최대화하는 의사 결정 전략을 학습하는 것</u>

> 어떻게 하면 의사 결정을 학습할 수 있을까?

→ 강화학습에서는 에피소드가 끝나면 지나왔던 상태에서 했던 행동에 대해 정보를 기록한다. 그리고 그 정보를 이용해 그 다음 에피소드에서의 의사 결정을 하는 것을 반복한다.

> 어떤 정보를 기록하면 좋은 의사 결정을 할 수 있을까?

→ 미래에 대한 정보를 미리 알면 좋은 의사 결정을 할 수 있을 것이다. 따라서 t 스텝에서 받았던 보상 $$R_{t+1}$$부터 에피소드가 끝날 때 까지 받았던 보상들을 더한 것을 정보로 사용한다.

- 감가율 $$\gamma$$ , 0 < $$\gamma$$ < 1 , 1에 가까울수록 미래의 보상에 많은 가중치를 두는 것을 의미한다.

---

### 가치 함수와 큐 함수

강화학습의 목적은 보상을 최대화하는 의사 결정 방법을 학습하는 것이다. 즉, <u>에이전트가 특정 상태에서 보상을 최대화할 수 있는 행동</u>을 선택해야한다.

> 보상을 최대화할 수 있는 행동이란?

→ 현재 상태에서 이동할 수 있는 다음 상태 중에서 <u>가장 가치가 높은 상태로 이동할 수 있는 행동</u>

> 가치가 높은 상태?

→ 그 상태의 반환값에 대한 기댓값이 높다.

이것을 수식으로 표현하기 위해 가치 함수라는 개념을 도입한다. 가치 함수에는 상태 가치 함수(State value function)과 행동 가치 함수(Action value function)이 있고 통상적으로 각각 value function과 Q함수로 부른다.

그리고 벨만 방정식을 통해 현재 상태의 가치 함수와 다음 상태의 가치 함수 사이의 관계를 표현할 수 있다.

### 탐험(exploration)과 이용(exploitation)

에이전트는 다양한 경험을 통한 학습을 거쳐 가장 최적의 정책을 학습할 수 있게 된다. 이렇게 에이전트가 다양한 경험을 할 수 있도록 에이전트의 행동을 결정하는 기법을 탐험이라고 하고, 학습한 결과에 따라 에이전트의 행동을 결정하는 기법을 이용이라고 한다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
