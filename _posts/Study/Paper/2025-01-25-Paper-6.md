---
title: "[논문] Deep Recurrent Q-Learning for Partially Observable MDPs"

categories:
  - Papers
tags:
  - [RL, DRQN]

toc: true
toc_sticky: true

date: 2025-01-26
last_modified_at: 2025-02-01
---

# 1. Abstract

Deep RL은 복잡한 문제에 대한 좋은 controllers를 생산해냈다.

하지만 이 controller는 <u>메모리가 제한적</u>이었고 각 결정 시점마다 완전한 게임 화면의 인식을 필요로했다.

이 문제점을 해결하기 위해 위 논문은 DQN의 first post-convolutional fully connected layer을 recurrent LSTM으로 대체해 recurrency를 추가한 효과를 조사했다.

Deep Recurrent Q-Network(DRQN)의 결과

1. 한 장의 frame으로도 성공적으로 정보를 통합할 수 있다.
2. DQN의 Atari게임에서의 성능과 동등하다.
3. flickering하는 partially observed에서도 동등한 feature를 얻음
    1. partially observation으로 학습하고 more complete observation으로 평가하면 DRQN의 성능이 observability에 비례해서 증가한다.
    2. 역으로 complete observation으로 학습하고 partially observation으로 평가해도 DQN보다 DRQN이 성능이 덜 떨어졌다.

⇒ 같은 길이의 history가 주어졌을 때, <u>DRQN이 DQN보다 학습시 나은 성능을 내는 것은 아니지만 recurrent net은 observation의 quality가 바뀌더라도 더 잘 적응할 수 있다.</u>

# 2. Introduction

DQN은 제한된 state(혹은 게임 화면)으로 학습하는데 한계가 있다. 그래서 DQN은 과거의 4 프레임보다 많이 기억해야 하는 게임은 마스터할 수 없다.

4프레임 이상의 기억이 필요한 게임은 non-Markovian으로 보일 수있다.

왜냐하면, 미래의 게임 State(및 보상)가 DQN의 현재 입력만으로 결정되지 않기 때문이다.

이러한 게임은 Partially-Observable Markov Decision Process (POMDP)라고 한다.

불완전한 State observation이 주어지면 DQN의 성능이 줄어드는 것을 확인했다.

DQN에 RNN을 활용해 POMDP를 더 잘 처리하게 수정할 수 있을 것이라고 가설을 세웠다.

그러므로 Long Short Term Memory(LSTM)과 DQN의 결합한 DRQN을 소개한다.

DRQN은

1. Partial Observability를 다룰수 있다
2. full observation으로 학습하고 partial observation으로 평가할 때 DRQN은 DQN보다 정보의 손실이 적다.

## Deep Q-Learning

![image.png](attachment:a2640255-990f-44e7-ac18-a557de711572:image.png)

Q-value는 위 식과 같이 구할 수 있다. 하지만 Atari 게임엔 각각의 Q-value를 업데이트하기에 너무 많은 State가 있다. 그래서 approximator를 사용한다.

![image.png](attachment:aca7900d-d579-4039-bd01-1acd6e494b62:image.png)

![image.png](attachment:9e048593-8720-4910-b3e2-a01305bb2905:image.png)

각각의 Q-value를 업데이트 하는 것 대신, loss function을 최소화하는 파라미터 $\theta$를 찾는 것으로 변경한다.

DQN은 학습 안정성을 위해 3가지 기술을 사용한다.

1. 경험 $e_t = (s_t, a_t, r_t, s_{t+1})$는 replay memory $D$에 저장되고 일정한 확률로 sampled 된다.
2. target network $\hat{Q}$는 main network와 동일하지만 $\theta^-$는 매 10,00번의 반복마다 $\theta$와 일치하도록 업데이트 된다.
3. RMSprop 또는 ADADELTA와 같은 Optimizer 기법을 활용해 각 파라미터에 대해 학습률 $\alpha$를 조정하고, 그 파라미터의 그라디언트 업데이트 기록을 통해 $\alpha$를 조정한다.

각 학습 iteration i에서 experience $e_t$는 replay memory $D$에서 랜덤하게 sample된다.

network의 Loss는 다음과 같이 정의된다. 

![image.png](attachment:a9cb1e6e-8c54-456c-92fb-7c7a82304825:image.png)

여기서 

![image.png](attachment:a82fa07c-6ebc-4372-8d70-c6cd93f3d3a3:image.png)

$y_i$는 target network  $\hat{Q}$에 의해 제공되는 업데이트 target이다.

이런 방식으로 업데이트 하는 것은 효과적이고 안정적이다.

# Stable Recurrent Updates

recurrent, convolutional network를 업데이트하려면 여러 시점의 게임 화면과 target value를 포함하는 역전파를 필요로 한다.

게다가, LSTM의 초기 hidden state는 0으로 초기화 되거나, 이전의 값을 받아 진행해야한다.

2개의 업데이트 방법이 있다.

1. Bootstraped sequential Updates:
    
    에피소드들은 replay memory에서 랜덤하게 선택되고 에피소드의 시작 부분에서 업데이트가 된다.
    각 time-step에서 targets는 target Q-network에서 발생된다. RNN의 은닉층은 episode 동안 계속 진행됨
    

1. Bootstraped random Updates
    
    에피소드들은 replay memory에서 랜덤하게 선택되고 에피소드의 랜덤한 부분에서 업데이트가 된다. RNN의 초기 state는 0에서 업데이트를 시작한다.
    

### Partial Observability

실제 환경에서 full state를 아는 것은 드물고, Markov property도 드물다.

POMDP가 실제 환경에서의 dynamic을 다루기 좋다.

POMDP는 $(S, A, P, R, \Omega, O)$로 나타낼 수 있다.

Agent는 더 이상 실제 시스템 State를 알 수 없고, Observation $o \in \Omega$를 받는다.

이것은 시스템 State에서 확률 분포인 $o$ ~ $O(s)$를 따라 발생한다.

vanilla DQN에서 POMDP의 상태를 해석하기 위한 매커니즘은 없고,  관찰이 기본 시스템의 state를 충분히 반영하는 경우에만 효과적이다.

일반적인 경우, 관찰로부터 Q값을 추정하는 것은 나쁠 수 도 있다.

$Q(o, a|\theta) \ne Q(s,a|\theta)$이기 때문이다. 

### DRQN Architecture

DQN의 first fully connected layer를 같은 사이즈의 recurrent LSTM layer로 대체한다.

![image.png](attachment:5bc57de8-84a7-40bb-ba90-37aa153c27b2:image.png)

### Stable Recurrent Updates

LSTM의 initial hidden state는 0으로 초기화 되거나 이전 값에서 이어서 사용할 수 있다.

- Bootstrapped Sequential Updates
    - Episode는 replay memory에서 무작위로 선택되고, episode의 시작 지점에서 업데이트가 시작되고, episode 종료시까지 carry forward하다.
    - target은 각 timestep마다 Q-network $\hat{Q}$로부터 생성된다.
    - RNN의 hidden state는 episode 동안 유지된다.
- Bootstrapped Random Updates
    - Episode는 replay memory에서 무작위로 선택되고, episode의 임의의 지점에서 시작되고, 일정시간만 진행된다.
    - target은 각 timestep마다 Q-network $\hat{Q}$로부터 생성된다.
    - RNN의 initial state는 update의 시작때 0으로 초기화된다.

### Atari Games: MDP or POMDP?

DQN의 input인 frames의 수를 줄이지 않으면서 partial observability를 확인할 방법이 필요하다.

### Flickering Atari Games

0.5의 확률로 fully obscured되거나 fully revealed되는 방식으로 Pong게임을 POMDP하게 할 수 있다.

Flickering Pong 게임을 성공하기 위해서 frames에서의 공의 위치나 속도, 패달의 위치같은 정보를 측정해 정보를 통합해야한다.

가장 중요한 부분은 게임의 화면을 통해 공의 속도를 알아내는 것이다.

DRQN의 higher-level recurrent layer를 통해 1 frame의 input으로도 좋은 성과를 냈다.

DRQN은 지난 10 timesteps에 대한 역전파를 통해 학습했다. 이를 통해 10 frames의 DQN과 1 frame의 DRQN은 같은 history를 얻을 수 있었다.

결과적으로, recurrent network는 정보를 통합할 수 있고, convolutation network에서 frame을 쌓는 것의 대안이 될 수 있다.

### Evaluation on Standard Atari Games

마지막 4 frame을 주었을때 게임들은 POMDPs가 아니라 MDPs이다. 그래서 DRQN이 DQN보다 나은 성능을 기대할 수 없다. 심지어 DRQN이 DQN보다 낫지도 않다.

⇒ MDP에서 DRQN의 성능을 기대하기 힘들다.

![image.png](attachment:b35df5ab-d976-461f-b1a7-f1e87b7c8069:image.png)

![image.png](attachment:a66325b5-7272-41cb-9e01-440d44677d74:image.png)

# MDP to POMDP Generalization

![image.png](attachment:9035ad3e-74e0-4292-ac86-b78f642bd4c8:image.png)

Table1에 있는 9개의 게임에 대해서 flickering을 추가해 다시 평가했다.

DRQN은 DQN보다 모든 flickering의 정도에서 이전 Observation probability의 score를 잘 유지한다(덜 떨어진다)

recurrent controller가 full state information으로 학습을 했어도 정보를 잃는 것에 대한 저항이 있다고 결론내림

# Discussion and Conclusion

이 논문은 LSTM과 DQN의 결합하는 방식으로 DQN을 수정해 POMDP의 특성인 noisy observations를 처리하려 했다.

DRQN의 결과로 각 단계의 1 frame만 보고도 정보간의 상관관계를 파악할 수 있었다.

Flickering Pong domain에서의 수행능력은 Observability에 비례하고, 모든 화면이 observed 되면 거의 완벽한 수준이다.

이 결과는 recurrent network가 잃어버린 게임 화면을 다룰정도로 충분히 robust하고 observablility의 증가에 따라 성능이 올라갈 수 있게 scalable하다는 것을 나타낸다.

DRQN은 Pong domain에서

standard Atari Games (full observation) 학습 → flickering Atari Games(Partialy observation) 평가 flickering 학습 → standard 평가
두 경우 모두 DQN보다 성능이 좋았다.

하지만 Pong은 실험한 게임들 중에서 이례적인(outlier) 사례로 보인다. 10개의 Flickering MDPs에서 recurrency를 사용해도 체계적인 성능 향상을 관찰하지 못했다.

마찬가지로, non-flickering Atari 게임들에서도 recurrent network를 사용하는 플레이어와 사용하지 않는 플레이어 간에 큰 차이가 없었다.

결론

recurrency는 state observation을 처리하는데 유효한 방법이다. 하지만 이를 사용하는 것이 convolutional network의 입력 계층에서 관측치를 단순히 쌓는(stacking) 것보다 체계적인 이점을 제공하지는 않는다는 것이다.

향후 연구 방향으로는, Pong과 Frostbite와 같이 특정한 게임에서 reccurent network가 더 나은 성능을 보이는 게임들의 관련 특징을 알아내는 것
[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}