---
title: "[논문] Deep Reinforcement Learning with Double Q-learning"

categories:
  - Papers
tags:
  - [RL, DDQN]

toc: true
toc_sticky: true

date: 2025-01-30
last_modified_at: 2025-01-30
---

# 1. Abstract

DQN같은 Q-learning 알고리즘은 특정 조건에서 action-value를 overestimate하는 것으로 알려졌다.

하지만 이 문제점에 대해 이전까지 다음과 같은 질문의 답을 알 수 없었다

1. overestimate가 왜 일어나는지?
2. overestimate가 일반적으로 일어나는 현상인지?
3. overestimate가 Q-learning 성능에 안좋은 영향을 주는지?
4. overestimate를 막을 수는 있는지?

이 논문에서는 이러한 질문에 대한 답을 제공한다.

# Introduction

> overestimate는 왜 발생할까?

Q-learning은 강화학습에서 가장 유명한 알고리즘 중 하나이지만, 때때로 비현실적으로 높은 action-value를 학습하는 것으로 알려져있다. 왜냐하면 그것은 underestimated value보다 overestimated를 선호하는 경향이 있는 action value를 maximization하는 단계를 포함하기 때문이다.

즉, DQN에서의 Q값(action value)은 추정치이기 때문에 오차가 존재할 수 있다. 그 추정치가 실제 값보다 overestimated되었다면 max연산에 의해 그것을 선택할 확률이 높다.

> overestimate는 무조건 성능에 부정적인 영향을 끼칠까?

overestimations가 발생했을 때 이게 무조건 성능에 부정적인 영향을끼치는 것만도 아니다.

1. 모든 values가 균일하게 높다면, 그것이 정책을 더 나쁘게 할것이라고 예상하지는 않는다.
2. optiministic한 것의 장점인 optimisim in the face of uncertainty같은 exploration technique도 있다.

하지만 overestimations는 균일하지 않고, state에 집중되어 있지도 않다. 그래서 정책의 퀄리티에 부정적인 영향을 끼칠수 있다.

> 실제로 overestimate가 발생을 하는가?

그렇다면 실제로 overestimations가 발생을 하는지 알아보기 위해서 DQN의 최상의 시나리오로 세팅을 하고 테스트를 해도 가끔씩 action-value overestimate가 일어나는 것을 확인했다.

> overestimate를 막기위한 해결책 : DDQN

tabular setting에서 처음 제안된 Double Q-learning 알고리즘은 function approximation와 deep neural network를 통해 일반화될 수 있다 이것을 활용해 만든 Double DQN을 보여준다.

DDQN은 더 정확한 value estimate를 계산하고, 몇 개의 게임에서 더 좋은 점수를 얻게했다.

이것은 DQN의 overestimate는 더 안좋은 정책을 만들고 그것을 줄이는 것이 유익하다는 것을 입증했다.


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}