---
title: "[논문] CONTINUOUS CONTROL WITH DEEP REINFORCEMENTLEARNING"

categories:
  - Papers
tags:
  - [RL, DDPG]

toc: true
toc_sticky: true

date: 2025-03-14
last_modified_at: 2025-03-14
---
# 0. Abstract

이 논문은 DQN의 성공적인 아이디어를 활용해 연속적인 action space로 확장할 수 있는 방법에 대한 내용이다.   
이를 위해 <u>Determinisitic Policy Gradient(DPG)를 기반으로 한 Actor-Critic 방식의 Model-free RL Algorithm인 Deep DPG (DDPG)를 제안</u>한다.

이를 통해 같은 학습 알고리즘, 신경망 구조, 하이퍼파라미터를 사용하여 20개 이상의 물리 시뮬레이션 문제를 성공적으로 해결했다.

DDPG를 통해 구한 정책의 성능은, 환경의 dynamics와 미분 값에 완전히 접근할 수 있는 planning algorithm과 비교했을 때도 경쟁력이 있고, 일부 문제에서는 end-to-end 학습도 가능하다는 것을 보여줬다.

# 1. Introduction

## 1.1 문제점과 해결방안

DQN은 고차원의 observation spaces를 가진 문제를 해결했지만, action space는 discrete하고 저차원인것만 다룰수 있었다. 하지만 <u>대부분의 문제는 연속적이고 고차원인 action space를 가지고 있었다.</u>

DQN은 Q function이 최대가 되는 행동을 찾는 것으로 동작하기 때문에 연속적인 경우 매 스텝마다 반복적인 최적화 과정을 필요로해 연속적인 영역에 바로 적용이 불가능하다. 

물론 DQN을 연속적인 영역으로 적용하기 위해서 action space를 discretize하는 방법이 있긴 하지만 이것은 action의 수가 급격하게 증가해 the curse of dimensionality를 갖는다는 한계가 있다.
예를 들어 7 자유도를 갖는 사람의 팔이 3개의 action을 할 수 있다면 차원은 $$3^7$$=2187이다.
이러한 상황은 action의 fine한 control을 요구하는 작업에는 맞지 않고, 그렇게 큰 action space는 효율적으로 explore하기도 어려워 DQN같은 네트워크로 학습하기가 어렵다.

이런 문제를 해결하기 위해 <u>이 논문에서 제안하는 model-free, off-policy actor critic 알고리즘은 고차원과 연속적인 action space에서도 정책을 학습할 수 있다.</u>
DPG에 기반한 알고리즘에 단순히 actor-critic을 적용했다고 해서 문제가 해결되는 것은 아니고, <u>actor-critic에 DQN의 아이디어를 더해 문제를 해결</u>했다.

DQN 이전에는 non-linear한 function approximators로 value function을 학습시키는 것은 어려웠다.
DQN은 value function을 그러한 function approximator를 사용해 안정적이고, 강건하게 사용할 수 있게 됐다.

<u>DDPG는 아래 두가지 DQN 아이디어에 batch normalization이라는 방법을 추가해 구현됐다.</u>

1. Replay buffer의 활용 (DQN)
2. Target network의 활용 (DQN)
3. Batch normalization

## 1.2 평가

DDPG 알고리즘을 평가하기 위해 다양한 도전적인 physical control problems를 설계했다.
problems에는 multi-joint, unstable contact dynamics, gait behavior, cartpole swing-up 같은 문제들이 있었다.
로봇 제어의 오랜 난제 중 하나는 비디오와 같은 raw sensory를 input으로 직접 정책을 학습하는 것이었다.
이를 위해 고정된 시점에서 카메라를 배치하고 모든 과제를 두 가지 방식으로 수행했다.

1. joint angles 같은 저차원 정보 사용
2. pixel같은 raw sensory를 그대로 사용

DDPG는 모든 실험에서 좋은 정책을 학습할 수 있었고 특히 저차원 정보(cartesian coordinates, joint angles)만으로도 학습이 가능했다.
또한 같은 하이퍼파라미터와 네트워크 구조를 유지한 상태에서도 pixel만을 기반한 학습도 가능했다.

이 접근법의 핵심은 간결함(simplicity)이다.
이것은 단순한 actor-critic 구조와 학습 알고리즘에 적은 moving parts로도 학습이 가능하다.
그리고 구현이 쉬워 더 어려운 문제나 대규모 네트워크에도 확장 가능하다.

Planner Algorithm은 항상 저차원 state space에서 수행하기 때문에 DDPG의 성능을 비교하기 위해 Planner를 기준으로 삼았는데 비교 결과 DDPG는 일부 실험에서 Planner보다 더 나은 정책을 찾았고, 특히 픽셀 기반 학습에서 Planner보다 성능이 좋은 경우가 있었다.

# 2. Background

Standard RL의 설정과 같이 에이전트가 discrete timesteps에서 환경 E와 상호작용하는 방식으로 구성된다.즉, 각 timestamp t 에서, 에이전트는 obseraction $$x_t$$ 를 받고, action $$a_t$$ 를 수행하며, scalar 보상 $$r_t$$ 를 받는다.

이 논문에서 고려한 모든 환경에서는 action이 실수(real-valued)이고 수학적으로는 $$a_t ∈ \mathbb{R}^N$$ 으로 표현된다. 일반적으로, 환경은 partially observed 할 수도 있어서, history를 통해 현재 상태를 설명해야 할 수도 있다. 하지만 이 논문에서는 fully observed한 환경으로 가정해 상태 $$s_t$$는 단순히 현재 관찰 값 $$x_t$$와 동일하다고 가정한다. <u>나머지 가정도 standard RL로 생각하면 될 것 같다.</u>

# 3. Algorithm

Q-learning은 continuous action space에 바로 적용하기 어렵다. 왜냐하면 greedy한 정책을 찾기 위해선 매 timestep마다 optimization을 해야하고, 이것은 큰 function approximators나 nontrivial action space에 적용하기에는 너무 느려 실용적이지 못하다.

이를 해결하기 위해 DPG 알고리즘에 기반한 Actor-critic 방법을 사용했다. 

1. Actor function $$\mu (s \mid \theta_\mu)$$
    - states를 입력으로 받아 deterministic 하게 specific action을 반환하는 함수
    즉, 하나의 action으로 직접 mapping이 된다.
2. Critic Q(s, a)
    - Q-learning처럼 벨만 방정식 사용으로 학습된다.

Actor의 업데이트는 actor의 파라미터에 관한 start distribution J로부터 기대되는 Return 값에 chain rule을 적용해 업데이트 한다.

![Image](https://github.com/user-attachments/assets/c402963a-6ef0-45fd-b505-f135942ab50a)

위의 식에 chain rule을 적용해 다음과 같이 나타낼 수 있고, 이것을 Policy gradient라 한다.

![Image](https://github.com/user-attachments/assets/109a31de-fa8d-4739-999c-47fd6200b310)

Policy gradient를 신경망같은 non-linear function approximator를 사용하면 수렴은 더이상 보장되지 않지만 큰 state space를 일반화하고 학습하기에는 필수적이다.

DQN의 성공적인 기법인 거대한 state, action space를 학습하기 위한 Neural network function approximators 사용을 DPG에 적용했고 이를 Deep DPG (DDPG) 라 부른다.

---

DDPG는 DQN의 아이디어인 Replay Buffer와 Target Network를 사용했다. 하지만 이중 Target Network와 비슷하지만 actor-critic에 맞게 변형한 Soft Target update를 사용한다.

actor와 critic의 copy인 $$Q'(s,a \mid \theta^{Q'}), \mu'(s \mid \theta^{\mu'})$$을 만들어 target value를 계산하는데 사용한다.

이러한 타겟 네트워크의 가중치는 다음과 같이 천천히 업데이트 된다.
$$\theta' \leftarrow \tau \theta + (1 - \tau) \theta'$$ , $$\tau \ll 1$$

이렇게 하면 target value의 변화가 완만해지고 학습 안정성이 크게 향상 된다.

이 작은 변화는 Q function의 상대적으로 불안정한 학습을 지도 학습의 경우와 비슷하게 만든다.

target $$\mu'$$ 와 $$Q'$$sms 발산없이 critic을 안정적으로 학습하기 위해선 안정적인 target $$y_i$$가 필요하다.

---

저차원의 feature vector observations으로 학습을 할 때, <u>observation의 다른 요소는 서로 다른 단위를 갖을 수 있다 (ex 위치 vs 속도).</u> 이러한 차이는 네트워크의 학습을 어렵게 만들고, 다른 크기의 state value를 갖는 환경에 맞는 하이퍼파라미터를 찾는것을 어렵게한다.

이 문제를 해결하기 위해 features가 비슷한 범위의 단위를 갖도록 feature를 scaling 해줘야 했고, 이를 위해Batch Normalization(BN)을 적용했다.

Batch Normalization은 미니배치 내의 sample의 차원이 단위 평균과 분산을 갖도록 정규화한다. 이것은 학습 과정에서 평균과 분산의 이동 평균을 유지해서 test시에도 (위 논문에선 evaluation과 exploration) 동일한 정규화를 유지한다. 이는 강화 학습에서 covariance shift를 최소화 하여 네트워크의 학습을 쉽게 만든다.

DDPG에서는 state input과 actor-critic network의 모든 층에 bacth normalization를 적용해서 다양한 환경에서도 추가적인 scaling 없이 학습 가능하도록 개선 했다.

연속적인 action space에서의 가장 큰 문제점은 exploration이다. 하지만 DDPG같은 off-policy 알고리즘의 장점은 학습 알고리즘으로부터 exploration을 분리할 수 있다는 것이다.

따라서, actor policy에 노이즈 N을 추가하여 exploration policy $$\mu'$$를 독립적으로 설계할 수 있다.

![Image](https://github.com/user-attachments/assets/4fb88036-a139-4eac-a75a-c751c9e028ae)

여기서 노이즈 N 는 환경에 맞게 선택이 가능하고, Ornstein-Uhlenbeck 과정을 사용해 시간적으로 연관된 exploration이 필요한 환경에서 효과적인 exploration을 수행했다.

---

# 4. Results

![Image](https://github.com/user-attachments/assets/2182822f-6bc4-4dd6-93f0-48c5e389b980)

MuJoCo에서 다양한 난이도의 시뮬레이션된 물리 환경을 구축하여 알고리즘을 테스트했다. 이 환경에는 cartpole 고전적인 강화학습 환경뿐만 아니라 gripper 같은 고차원 제어 작업 puck striking 같은 접촉을 포함하는 작업, cheetah 같은 locomotion작업이 포함되었다.

TORCS 레이싱 게임도 진행했는데 다른 환경과 동일한 신경망 구조 및 학습 하이퍼파라미터를 사용했고 noise process의 경우 time scale이 다르므로 조정을 했다.

그 결과 서킷을 완주할 수 있는 정책을 학습한 것도 있었지만 일부에서는 제대로 된 정책을 학습하지 못했다.

![Image](https://github.com/user-attachments/assets/8728ae3c-7053-4c45-aad0-7e8b275b6040)

학습 중 주기적으로 exploration noise없이 정책을 평가했다.

평가 결과로 target network 그리고 Batch normalization이 없는 대부분의 작업에서 성능이 크게 저하됐다. 특히, 원래 DPG에서 사용된 방식처럼 target network 없이 학습할 경우, 많은 환경에서 학습이 거의 되지 않거나 성능이 매우 좋지 않은 것을 확인할 수 있다.

![Image](https://github.com/user-attachments/assets/3629d4ac-5e4f-4735-9ade-dcef48990c77)

![Image](https://github.com/user-attachments/assets/42ab78e0-4be8-455b-9829-0a81b08eacc6)

위 표는 DDPG가 다양한 환경에서 보인 성능을 요약한 것이다.

각 점수는 5개의 독립적인 실험의 평균 점수성능을 평가하기 위해 두 개의 baseline을 설정했다.

1. Naive Policy :  action을 uniform distribution에서 sampling (랜덤)인 경우 점수 : 0
2. iLQG (Iterative Linear-Quadratic Gaussian)인 경우 점수 : 1

결과를 보면 DDPG는 대부분의 작업에서 좋은 정책을 학습했으며, 일부 실험에서는 iLQG보다 높은 성능을 보이기도 했다. 특히, 픽셀 데이터만 사용한 학습에서도 iLQG를 능가하는 경우가 있었다.

![Image](https://github.com/user-attachments/assets/c920a07c-55f1-4182-aecd-353e3bf76d94)

Q-learning은 Q-value를 overestimation하는 경향이 있다

이를 검증하기 위해, 학습 후 Q-function이 예측한 값과 실제 테스트 에피소드에서 얻은 return을 비교했다.

Pendulum이나 Cartpole같은 단순한 작업에서는 Q-value이 실제 값에 어느정도 정확한 estimation을 보였지만, Cheetah 같은 복잡한 작업에서는 Q-value의 estimation 정확도가 떨어졌다. 그럼에도 DDPG는 여전히 좋은 정책을 학습할 수 있었다.

# 5. Concolusion

이 논문은 Target Network와 Replay buffer 그리고 Batch normalization을 더해 연속적인 Action Space를 갖는 다양한 문제를 해결할 수 있는 강건한 알고리즘인 DDPG를 제안한다.

DDPG는 일부 환경에서 riw pixel 만으로도 효과적으로 학습이 가능한 것을 확인했다. 또한 대부분의 RL 알고리즘이, non-linear function approximators를 사용하기 때문에 수렴 보장성은 없지만, 실험 결과는 환경 변화 없이도 안정적인 학습이 가능함을 보여준다.

모든 실험은 AtariGame에서 DQN 보다 적은 Step만으로 동작했다. 거의 모든 문제는 2.5m experience 이내로 동작했고 이것은 DQN이 좋은 성능을 내는데 필요한 experience의 1/20 수준이다. 그렇기 때문에 DDPG가 시뮬레이션 시간을 더 보장받는다면, 더 어려운 문제도 해결할 수 있을것이라고 생각한다.

DDPG에게도 한계점이 있는데, DDPG는 대부분의 model-free가 그런것처럼 많은 양의 학습 에피소드가 필요하다는 점이다. 하지만 DDPG와 같은 강건한 model-free 방법은 더 큰 시스템의 중요한 요소가 될 수 있을 것이라고 생각한다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
