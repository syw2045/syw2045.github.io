---
title: "[논문] Proximal Policy Optimization Algorithms : PPO"

categories:
  - Papers
tags:
  - [RL, PPO]

toc: true
toc_sticky: true

date: 2025-03-21
last_modified_at: 2025-03-21
---
# 0. Abstract

환경과의 상호 작용을 통해 데이터를 샘플링하는 단계와 stochastic gradient ascent를 사용해 surrogate를 최적화하는 단계를 번갈아 수행하는 RL policy gradient 기반 알고리즘을 제안한다.

기존 policy gradient 방법은 각 데이터 샘플에 대해 한 번만 경사를 업데이트하는 반면, 제안하는 알고리즘은 미니배치(minibatch) 업데이트를 여러 번 수행할 수 있도록 새로운 목적 함수를 설계했다.

Proximal Policy Optimization (PPO)은 TRPO의 여러 장점을 유지하면서도 구현이 간단하고 범용적으로 사용할 수 있으며, 데이터 효율성도 더 뛰어나다.

PPO는 다른 online policy gradient 방법보다 우수한 성능을 보이며, 데이터 효율성, 단순성, 실행 시간 간의 균형을 효과적으로 유지하는 것이 확인되었다.

---

# 1. Introduction

Deep Q-learning, Vanilla Policy Gradient, TRPO 등 다양한 강화학습 기법들이 등장했지만, 확장성(scalability), 데이터 효율성(data efficiency), 그리고 robustness 측면에서 여전히 보완할 부분이 있었다.

일단, Q-learning은 간단한 문제에서도 실패하는 경우가 많았고, Vanilla Policy Gradient는 데이터 효율성과 안정성 면에서 한계를 보였다. TRPO는 비교적 안정적인 성능을 내지만, 알고리즘이 복잡하고 noise가 많거나 parameter sharing을 포함한 구조에서는 사용하기 어렵다는 단점이 있었다.

PPO는 TRPO의 데이터 효율성이나 안정적인 성능을 유지하면서 first-order optimization만을 사용해 이러한 단점을 해결하고자 한다.

PPO는 probability ratio를 clipping한 새로운 목적 함수를 제시하며, 정책 성능의 하한(lower bound)을 설정한다. 최적화를 위해 정책에서 데이터를 샘플링하는 단계와, 샘플링된 데이터를 여러 epoch 동안 학습하는 단계를 번갈아 수행하는 방식을 채택했다.

또한 여러 버전의 surrogate objective를 비교한 결과, probability ratio를 clipping한 방식이 가장 우수한 성능을 보였다.

PPO를 기존 알고리즘과 비교한 결과, 연속 제어(continuous control) 환경에서 PPO가 다른 알고리즘보다 뛰어난 성능을 보였으며, Atari 게임에서는 A2C보다 샘플 효율성이 훨씬 높았다. 또한 ACER와 유사한 성능을 내면서도 구현이 훨씬 간단한 장점을 가졌다.

---

# 2. Background: Policy Optimization

## 2.1 Policy Gradient Methods

policy gradient 방법은 policy gradient의 추정치를 계산해 stochastic gradient ascent 알고리즘 적용한다. 가장 많이 사용하는 policy gradient estimator는 다음과 같다.

![Image](https://github.com/user-attachments/assets/303ee4c2-90de-46d0-bb25-b4e7faa9dfd4)

$$\pi_\theta$$ : stochastic policy

$$\hat{A}_t$$ : t일때 advantage function estimator

$$\hat{E}_t$$ : 샘플링과 최적화를 반복한 알고리즘을 통해 얻은 유한한 배치 샘플의 평균 

policy gradient estimator $$\hat{g}$$의 gradient를 계산할 수 있는 목적 함수는 다음과 같다. 

![Image](https://github.com/user-attachments/assets/65009d1a-3f28-47d1-ab91-c6f347ccb13a)

이 목적 함수를 사용해 최적화를 여러 번 수행하는 것이 직관적으로 타당해 보이지만, 실제로 이 방법은 이론적으로도 맞지 않고, 지나치게 큰 정책 업데이트로 인해 학습이 불안정해지는 문제가 발생한다.

## 2.2 Trust Region Methods

TRPO에서 목적함수 surrogate는 정책 업데이트 크기에 constraint를 두면서 surrogate 목적 함수를 최대화 하는 방법을 사용한다.

![Image](https://github.com/user-attachments/assets/b7e35398-14c3-4670-ac1a-6d44c85c92e1)

$$\theta_{old}$$ : 업데이트 전의 정책 파라미터

KL : Kullback-Leibler divergence

$$\delta$$ : 정책 변화에 대한 Trust Region

이 문제는 목적 함수에 대한 linear approximation 그리고 constraint에 대한 quadratic approximation를 만든 다음 conjugate gradient 알고리즘을 사용하여 근사적으로 해결할 수 있다.

이론상으로는 TRPO가 constraint 대신에 패널티를 사용하는 것이 더 적절하다. 즉, unconstraint 최적화 문제를 푸는 것이 더 적절하다. 

![Image](https://github.com/user-attachments/assets/b9f94ff3-1392-49bd-b233-7f74c5a8a00e)

$$\beta$$ : KL divergence에 대한 패널티 계수

TRPO는 패널티가 아닌 hard constraint를 사용하는 것은 다양한 문제에서 좋은 성능을 내는 $$\beta$$를 선택하는 것이 어렵기 때문이다. 심지어 한 문제 내에서도 학습 진행에 따라 적절한 $$\beta$$가 달라질 수 있다.

그래서 TRPO의 단조적인 성능 향상을 first-order algorithm로 구현하는 것이 목표라면 고정된 값인 $$\beta$$를 사용하는 것과 SGD로 penalized objective equation을 최적화하는 것만으로는 충분하지 않고, 추가적인 수정이 필요하다. 

---

## 3. Clipped Surrogate Objective

![Image](https://github.com/user-attachments/assets/2e8cda30-b693-4ed9-95cf-75a7cc401c82)

$$r_t(\theta)$$를 위와 같이 정의하면 $$r(\theta_{old})$$ = 1이 성립한다. 

![Image](https://github.com/user-attachments/assets/ef7bf47c-18ae-44c9-bc6f-da8525347909)

TRPO는 surrogate 목적함수를 최대화 한다. (CPI : Conservative Policy Iteration)

constraint가 없는 $$L^{CPI}$$ 최대화는 과도한 정책 업데이트를 유발할 수 있다. 그래서 정책이 너무 급격히 변화하지 않도록 $$r_t(\theta)$$가 1에서 멀어지는 것을 억제하는 방식으로 목적 함수를 수정해야 한다.

![Image](https://github.com/user-attachments/assets/c069e83e-b0da-444d-81d9-875a97d23dbf)

이 논문에서 제안하는 목적 함수는 위와 같다.

min함수 안에 있는 첫 번째 항 $$r_t(\theta)\hat{A}_t$$는 TRPO의 surrogate 목적함수인 $$L^{CPI}$$와 같다. 그리고 두 번째 항 $$clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t$$은 surrogate 목적 함수를 수정해 clipping을 통해  $$r_t(\theta)$$가 $$[1-\epsilon , 1+\epsilon]$$ 범위를 벗어나지 않도록 조정한다.

즉, 정책 업데이트가 너무 급격하게 변화하지 않도록 cliping해 constraint를 가한다. 최종적으로 clipped된 것과 unclipped된 것의 min을 통해 계산한다. 

$$L^{CLIP}(\theta)$$ 와 $$L^{CPI}(\theta)$$ 는 $$\theta_{old}$$ 주변의 first order는 비슷하다. 하지만 $$\theta$$가 $$\theta_{old}$$에서 멀어질 수록 두 목적함수는 달라지게 된다.

---

![Image](https://github.com/user-attachments/assets/9adb6e2c-c34d-4b32-ad89-aa8a4f0dfd49)

$$1-\epsilon$$과 $$1+\epsilon$$을 결정하는 것은 A의 부호이다. 

![Image](https://github.com/user-attachments/assets/901db641-9a21-430a-945d-9eecdf4ba0f5)

Continuous control problem에서 PPO를 사용했을 때 다양한 목적 함수를 나타낸 것이다

$$L^{CLIP}$$이 $$L^{CPI}$$의 하한임을 확인할 수 이다.

---

# 4. Adaptive KL Penalty Coefficient

clipped surrogate의 대안으로 사용하거나 추가해 사용할 수 있는 방법은 penalty on KL divergence이다. 매번 정책을 업데이트할 때 마다 KL divergence의 target value인 $$d_{targ}$$를 달성하기위해 penalty coefficient를 변경한다.

KL Penalty 방식은 clipped 방식보다 성능은 좋지 않았지만 중요한 baseline이다.

![Image](https://github.com/user-attachments/assets/79408969-549e-4d46-ba5e-c219276772cf)

$$d$$와 $$d_{targ}$$를 비교해 업데이트 된 $$\beta$$는 다음 정책 업데이트에 사용된다. 이 방식에서는 KL divergence($$d$$)가 $$d_{targ}$$과 차이가 큰 경우가 가끔 발생하지만, 이는 드물며 $$\beta$$가 빠르게 조정되기 때문에 문제가 되지 않는다.

그래서 알고리즘의 성능도 $$\beta$$에 크게 민감하지 않다.

---

# 5. Algorithm

surrogate loss 함수는 일반적인 Policy Gradient 구현에 약간의 수정으로 계산하고 미분할 수 있다. 자동으로 미분을 해주는 라이브러리를 사용하는 경우에는 $$L^{PG}$$ 대신 $$L^{CLIP}$$ 혹은 $$L^{KLPEN}$$을 사용하고 이 목적함수에 Stochstic Gradient Ascent를 여러번 수행하면 된다.

variance reduced를 적용하려면 GAE, finite-horizon estimators 같은 advantage-function을 추정하기 위한 V(s)를 학습해야 한다.

만약 정책과 value-function이 같은 신경망에서 파라미터를 공유한다면, surrogate loss와 value function error term을 합친 손실함수를 사용해야 한다.

또한 충분한 exploration을 위해 entropy bonus를 추가할 수도 있다. 이러한 요소들을 합쳐서 최적화할 목적함수를 아래와 같이 정의할 수 있다.

![Image](https://github.com/user-attachments/assets/87a009f5-62d7-41bf-b771-6152d9567df6)

$$S[\pi\theta(s_t)]$$ : entropy bonus

$$L^{VF}$$ : value-function의 Loss / squared-error loss

c1, c2 : entropy bonus와 value-function Loss 의 계수

---

A2C 알고리즘에서는 T timesteps 동안 실행해 데이터를 모으고 이를 이용해 학습한다.

이러한 방식은 T step 이후의 미래 정보를 사용하지 않는 advantage estimator가 필요하다. A2C에서 사용한 방법은 아래와 같다.

![Image](https://github.com/user-attachments/assets/8402e161-a76d-406e-8b80-e552aa6f1fb0)

즉, 고정된 길이 T만큼만 정보를 활용하는 advantage estimate 방식이다. 이를 일반화 하면 GAE의 형태로 변형이 가능하다.

![Image](https://github.com/user-attachments/assets/b7fad087-de31-4ddf-9d3b-96dbea710982)

---

PPO 알고리즘은 여러 개의 actor를 사용해 데이터를 수집하고 최적화한다.

![Image](https://github.com/user-attachments/assets/8107f4f2-bde8-47eb-81e2-c884ab8d7aa8)

1. N개의 Actor가 각각 T timesteps 길이의 데이터를 env로부터 수집한다.
2. NT timestep의 데이터로 surrogate loss를 만든다.
3. K epochs 동안 minibatch SGD나 Adam으로 최적화한다.
4. 새로운 정책으로 다음 iteration 진행

---

# 6. Experiments

## 6.1 Comparison of Surrogate Objectives

먼저 여러 surrogate objectives를 서로 다른 하이퍼파라미터로 설정해 성능을 비교했다. 7개의 MuJoCo 환경에서 실험을 진행

![Image](https://github.com/user-attachments/assets/7e8499c8-cf6f-45ec-b07c-278be7717820)

![Image](https://github.com/user-attachments/assets/d2bb7729-c085-4886-be70-29a0c580a4ba)

성능을 비교한 표를 보면 Clipping 하는 것이 KL Penalty 보다 좋은 것을 확인할 수 있다. 

## 6.2 Comparison to Other Algorithms in the Continuous Domain

![Image](https://github.com/user-attachments/assets/e291810f-21aa-4b82-8ec0-910a91dfdb7f)

PPO 알고리즘을 Continuous Domain에서 다른 알고리즘과 비교한 그래프.   
그래프를 보면 PPO Clip (보라색)이 여러 Domain에서 좋은 성능을 내는 것을 확인할 수 있다. 

## 6.3 Showcase in the Continuous Domain: Humanoid Running and Steering

PPO 알고리즘을 고차원의 continuous control problems에서 확인하기 위해 3D 휴머노이드 로봇을 움직이는 실험을 했다.

실험은 세 가지로

1. RoboschoolHumanoid : 앞으로 가기
2. RoboschoolHumanoidFlagrun : 200 step을 동작하면 Goal의 위치가 변경
3. RoboschoolHumanoid-FlagrunHarder : 로봇이 넘어질수도 있어  스스로 일어나야됨

![Image](https://github.com/user-attachments/assets/1399a67b-054f-4bb8-b4ee-3d91c04a0be5)

그래프를 통해 고차원의 continuous control problems에서도 좋은 성능을 확인할 수 있다.

![Image](https://github.com/user-attachments/assets/ce213f2a-220c-4637-8c93-4af2421ef444)

## 6.4 Comparison to Other Algorithms on the Atari Domain

PPO 알고리즘의 성능을 Atari Game 에서도 평가하기 위해서 A2C, ACER와 비교했다. 세 알고리즘은 모두 같은 policy network architecture를 사용하고, A2C, ACER의 하이퍼파라미터는 가장 좋은 성능을 내는 것으로 선택했다.

![Image](https://github.com/user-attachments/assets/a17c4ba6-81ab-474b-83d6-bc8e09d962f8)

평가 지표는 학습 전체 동안의 총 reward와 마지막 100 episode의 reward를 비교했다.

위의 표에서 PPO는 49개의 Atari Game중 30개의 게임에서 전체 reward가 제일 좋았다.

---

# 7. Conclusion

Proximal Policy Optimization(PPO)알고리즘은 매 policy update 마다 여러 epochs의 stochastic gradient ascent를 하는 방식이다.

이 방식은 TRPO의 안정성과 신뢰성을 유지하면서도 훨씬 간단하게 구현할 수 있고, vanilla policy gradient에서 몇 줄의 코드만 바꿔 구현할 수 있다. 또한 더 일반적인 환경에서도 사용할 수 있고, 전반적인 성능이 더 좋다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
