---
title: "[논문] The StarCraft Multi-Agent Challenge"

categories:
  - Papers
tags:
  - [MARL, SMAC]

toc: true
toc_sticky: true

date: 2025-05-02
last_modified_at: 2025-05-02
---

# 0. Abstract

Deep MARL은 활발한 연구가 되고 있는 분야이다. 특히 그중에 주목받고 있는 분야는 Partially observable, cooperative, multi-agent learning이다. 각 Agent는 private observations에만 기반해 행동을 선택해야한다. 이러한 문제는 실제 시스템에 적용이 가능하고 general-sum 문제에 비해 평가가 용이하다는 점에서 매력적인 연구 주제이다.

Single agent RL는 ALE나 MuJoCo같은 표준화된 환경이 있지만 cooperative multi-agent RL의 경우 벤치마크가 없어 일회성의 실험만 진행해야했다. 그로인해 실질적인 연구가 나아가기 힘들었다.

이러한 문제를 해결하기 위해 StarCraft Multi-Agent Challenge (SMAC) 벤치마크를 제안한다. 각 유닛은 독립적인 Agent로 제어되고, local observation 만으로 행동한다.

---

# 1. Introduction

## 1.1 Why MARL is needed?

Deep RL은 사용자가 행동을 명확히 표현할 수 있는 보상 함수를 세울수만 있으면 순차 의사결정 문제를 해결할 수 있다는 점에서 많은 주목을 받았다. 하지만 <u>실제 세계의 문제는 MARL적인 특성을 갖는다.</u> 따라서 decentralisation constraints와 많은 agents의 지수적으로 증가하는 joint action space를 해결할 수 있는 MARL solution의 개발이 필수적이다.

그 중에서도 partially observable, cooperative, multi-agent learning 문제는 흥미로운 연구 분야다. cooperative 문제는 general-sum 게임의 평가의 어려움을 피할 수 있고, 하나의 사용자가 전체 goal을 구체화할 수 있는 distributed system을 관리하는 문제에도 적합하다. 실제 세계에서는 센서의 제한으로 partial observability가 있어서 <u>효율성이 중요</u>하고 커뮤니케이션의 한계도 있어서 <u>학습된 정책의 decentralised execution을 필요로한다.</u>

<u>제어된 환경이나 시뮬레이션에서의 학습은 추가 정보를 사용할 수 있어 이러한 문제에서 벗어날 수 있다.</u>

## 1.2 Why Benchmark is needed?

Single Agent RL 분야에서는 ALE나 MuJoCo와 같이 표준화된 환경이 있어 연구에 큰 진전이 있었다. 하지만 MARL 분야에서는 표준화된 벤치마크가 없다는 문제가 있다. 이로 인해 <u>연구자들은 각자가 생각해낸 환경에서 일회성 실험을 진행해야 하고 이 환경은 지나치게 단순하거나 제안된 알고리즘에 최적화되어 있을 수 도 있다는 문제</u>가 있다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
