---
title: "[논문] Attention Is All You Need"

categories:
  - Papers
tags:
  - [AI, Transformer]

toc: true
toc_sticky: true

date: 2025-05-08
last_modified_at: 2025-05-08
---

![alt text](image.png)

1. 기존 RNN 기반 모델은 순차적으로만 계산이 가능해 긴 sequence 처리에 한계가 있었다.
2. 이를 해결하기 위해 병렬 처리가 가능한 Transformer 아키텍처를 제안한다.
3. Transformer는 기존 방식의 Attention + Recurrency 조합이 아닌, 오직 Attention 메커니즘만을 활용한다

# 0. Abstract

기존의 주요 sequence transduction 모델들은 인코더와 디코더를 포함하는 RNN이나 CNN에 기반했다. 가장 성능이 뛰어난 모델들 또한 인코더와 디코더를 Attention 메커니즘으로 연결하기도 한다.

Attention 매커니즘만을 기반으로 하고 recurrence와 convolutions를 완전히 배제한 새로운 간단한 네트워크 구조인 Transformer를 제안한다.

두가지 번역 작업에 대한 실험 결과 이 모델은 품질 면에서 더 뛰어나고 병렬 처리에 더 적합하고 학습 시간도 훨씬 적게 필요하다는 것을 보여준다.

---

# 1. Introduction

RNN, LSTM, GRU는 sequence modeling과 transduction problem에서 SOTA를 기록했었다. 하지만 RNN 계열의 recurrent 모델은 일반적으로 입력 및 출력 sequence의 position을 순차적으로 계산하기 때문에 문장을 병렬로 한번에 처리하지 못한다는 단점을 지닌다. 이렇게 되면 길이가 긴 sequence의 경우 메모리의 제한으로 Parallelization이 불가능해 처리에 한계가 있었다.

이를 해결하기 위해 연산 효율성을 높이는 factorization이나 conditional computation 같은 방법이 연구가 됐고 이를 활용해서 어느 정도 개선을 했지만 순차적인 계산에 대한 근본적인 한계가 남아있었다. 따라서 이 논문은 Transformer라는 새로운 모델 아키텍처를 제안한다. 이 모델은 recurrency는 완전히 배제하고 Attention 메커니즘만 사용을 한다. Attention은 입력 또는 출력 sequence 내에서 거리와 무관하게 의존 관계를 모델링할 수 있어 global한 의존 관계도 모델링이 가능하다.

---

# 2.Background

sequential computation을 줄이려는 목표는 Extended Neural GPU, ByteNet, ConvS2S등의 모델에서도 같다. 이들은 CNN을 기본으로 하고 입력과 출력 sequence의 모든 position에 대해 hidden representation을 병렬로 계산한다.

하지만 위 모델들에서 임의의 두 입력 또는 출력 position 간의 신호를 연결하기 위해 필요한 연산 수가 position간의 거리만큼 증가한다. (ConvS2S : linearly, ByteNet : logarithmically). 이러면 거리가 먼 position은 의존성을 학습하기가 어렵다.

반면 Transfomer는 연산량이 상수로 줄어들고, attention-weighted의 평균으로 해상도가 낮아질 수는 있지만 이것은 Multi-Head Attention으로 보완한다.

Self-Attention(Intra-Attention)은 sequence의 representation을 계산하기 위해 하나의 sequence의 서로 다른 positions를 연결한다. reading comprehension, abstractive summarization, textual entailment 등의 Task에서 성공적으로 사용되었다.

End-to-end memory networks는 sequence-aligned recurrence 대신에 recurrent attention 메커니즘에 기반한다. simple-language 문답과 언어 모델링에서 좋은 성능을 보인다.

Transformer는 입력과 출력을 sequence-aligned된 RNN이나 CNN을 전혀 사용하지 않고 self-attention만 사용한 최초의 transduction model이다.

---

# 3. Model Architecture

![alt text](<../../../assets/images/Study/Paper/20/image 1.png>)

## 3.1 Encoder and Decoder Stacks

인코더(Encoder)

- 6개의 identical layers
- 각 layer는 2개의 sub-layer로 구성된다. 첫번째는 multi-head self-attention 메커니즘이고 두번째는 position-wise fully connected feed-forward network이다.
- sub-layer에는 residual connection이 적용되고, 후에 layer normalization이 진행된다.
- residual connection을 위해 모든 sub-layer와 embedding layer는 $$d_\text{model} = 512$$ 차원을 유지한다.

디코더(Decoder)

- 6개의 identical layers
- 인코더의 sub-layer에 하나를 더한다. (인코더 스택의 출력에 대해 multi-head attention을 수행)
- sub-layer에는 residual connection이 적용되고, 후에 layer normalization이 진행
- 디코더 스택의 self-attention sub-layer를 수정해 Masking 처리를 한다. Position i에 대한 예측이 i보다 작은 position의 출력만 참고하도록 Masking을 한다.

## 3.2 Attention

Attention function은 query와 key-value 쌍을 출력으로 매핑하는 방식이다. 출력은 values의 weighted sum으로 계산되고 각 value에 부여되는 가중치는 query와 key 간의 compatibility function에 의해 결정된다.

### 3.2.1 Scaled Dot-Product Attention

![alt text](<../../../assets/images/Study/Paper/20/image 2.png>)

`Scaled Dot-Product Attention`의 입력은 $$d_k$$ 차원의 query와 key 그리고 $$d_v$$ 차원의 value로 구성되어있다. query와 key에 내적을 계산한 후 각 값을 $$\sqrt{d_k}$$로 나누고 softmax를 적용해 value의 가중치를 얻는다.

실제로는 여러 query를 동시에 계산하기 위해 matrix Q로 묶고, key와 value도 각각 matrix K, V로 묶어 아래와 같이 계산한다.

![alt text](<../../../assets/images/Study/Paper/20/image 3.png>)

자주 사용되는 Attention 함수는 `additive attention`과 `dot-product attention`이다.

`dot-product attention`은 위에 설명한 scaled dot-product attention에서 $$\sqrt{d_k}$$로 나누는 것이 없는 형태이고 `additive attention`은 하나의 은닉층을 갖는 feed-forward network를 통해 compatibilty function을 계산하는 형태이다.

두 방식의 이론적 복잡도는 비슷하지만 dot-product attention은 최적화된 행렬곱 코드로 구현이 가능해 더 빠르고 메모리 효율성이 높다.

$$d_k$$차원의 작은 value에서는 두 방식이 비슷하지만, $$d_k$$차원의 큰 value에서는 additive attention이 scaling이 없는 dot-product attention보다 좋은 성능을 낸다. 왜냐하면 큰 value에서는 dot-product의 값이 커져서 softmax 함수가 극도로 작은 gradient를 갖는 영역으로가기 때문이다. 이것을 막기위해 dot-product 값을 $$\sqrt{d_k}$$로 나눠 scaling 한다.

### 3.2.2 Multi-Head Attention

![alt text](<../../../assets/images/Study/Paper/20/image 4.png>)

$$d_{model}$$-dimensional keys, values, queries가 attention function을 한 번 거치는 것 보다 각자 다르게 학습된 linear projection을 h번 하는 것이 더 효과적이다. 이때 attention function은 병렬로 수행할 수 있고 $$d_v$$-dimensional 출력을 만들어낸다.

![alt text](<../../../assets/images/Study/Paper/20/image 5.png>)

Multi-head attention는 서로 다른 position의 subspace에서 정보를 모아 만들어진다. Multi-Head Attention은 Scaled Dot-Product Attention 여러 개 Concat 해 구성된다.

각 head에서 차원이 줄기 때문에 전체적인 계산 cost는 single-head attention과 비슷하다.

### 3.2.3 Applications of Attention in our Model

Transformer는 multi-head attention을 세가지 다른 방법으로 사용한다.

- encoder-decoder attention Layer에서 query는 디코더 layer 전에서 오고, Key와 Value는 인코더의 출력에서 받아온다. 이것은 디코더의 모든 position에서 input sequence의 모든 position을 고려할 수 있게 한다.
- 인코더는 self-attention layers를 포함한다. self-attention layer에서는 query, key, value가 같은 곳에서 나온다. 즉, 인코더의 이전 layer 출력에서 가져오고 인코더의 각 position은 인코더 전의 모든 position을 고려한다.
- 디코더도 self-attention layers를 포함한다. 디코더의 각 position이 그 position을 포함한 모든 위치에 집중할 수 있다. 하지만 디코더의 auto-regressive 특성을 유지하기 위해 leftward information flow를 막기위한 Masking이 필요하다. Masking은 scaled dot-product attention에서 softmax 입력 중 연결이 불가능한 모든 value에 $$-\infin$$로 설정한다.

## 3.3 Position-wise Feed-Forward Networks

인코더와 디코더의 각 Layer에는 Fully Connected Feed-Forward Network가 있다. 이때 각 position 마다 독립적으로 동일하게 적용되기 때문에 position-wise라고 한다.

![alt text](<../../../assets/images/Study/Paper/20/image 6.png>)

이 네트워크는 두개의 선형 변환 사이에 ReLU 함수를 사용하는 구조이다. 이때 선형 변환은 position마다 동일하지만 layer마다 다른 파라미터를 사용한다.

입력과 출력의 차원: $$d_{model} = 512$$이고 inner-layer의 차원: $$d_{ff} = 2048$$이다.

## 3.4 Embeddings and Softmax

다른 sequence transduction 모델과 같이 입력 token과 출력 token을 $$d_{model}$$ 차원의 벡터로 변환하기 위해 학습된 embedding을 사용한다. 또한 디코더 출력을 next-token의 확률 예측으로 변환하기 위해 선형 변환과 softmax 함수를 사용한다.

Transformer 모델은 두 embedding layer와 pre-softmax 선형 변환끼리 동일한 가중치 matrix를 공유한다. embedding layer에서는 이 가중치에 $$\sqrt{d_{model}}$$을 곱해 사용한다.

## 3.5 Positional Encoding

Transformer 모델은 Recurrence나 Convolution을 사용하지 않기 때문에 sequence 내에서 토큰의 순서를 모델이 인식할 수 있도록 상대적 또는 절대적 위치 정보를 넣어줘야한다. 이를 위해 인코더와 디코더 스택의 입력 embedding 하단에 positional encoding을 더한다.

positional encoding은 embedding과 동일한 차원인 $$d_{model}$$을 가져 두 값을 더할 수 있다. 

![alt text](<../../../assets/images/Study/Paper/20/image 7.png>)

pos는 position을 의미하고 i는 dimension을 의미한다. 위 함수를 선택한 이유는 특정 offset k에 대해 $$PE(pos + k)$$가 $$PE(pos)$$의 선형 함수로 표현될 수 있어 상대적 position 정보를 쉽게 학습할 수 있을 것이라고 판단했다.

---

# 4. Why Self-Attention

recurrent와 convolution이 아닌 self-attention을 사용해야 하는 이유에 대해서 설명한다.

1. layer 당 전체 계산 복잡도
2. 병렬화가 가능한 연산의 양 (최소 sequential 연산량)
3. long-range dependencies간의 path length

sequence transduction 작업에서 long-range dependencies를 학습하는 것은 중요하다. 이런 의존성을 쉽게 학습하기 위해선 입력과 출력 간의 path length가 짧아야 하고 이를 통해 forward/backward 신호가 효율적으로 전달될 수 있다.

![alt text](<../../../assets/images/Study/Paper/20/image 8.png>)

Self-Attention은 sequential operations가 $$O(1)$$인 반면 Recurrent는 $$O(n)$$을 필요로 한다. 또한 sequence 길이 $$n$$이 representation 차원 $$d$$보다 작으면 Self-Attention이 Recurrent보다 빠르다.

Single convolutional layer의 kernel width가 $$k < n$$일 때 모든 입력-출력 position을 연결하지는 못한다. 이를 위해 O(n/k)개의 convolutional layers O(logk(n)) 개의 dilated convolution가 필요하다. 이를 통해 네트워크 내의 두 position 간의 최대 path length를 증가시킨다.

Convolutional layers는 일반적으로 recurrent layer보다 kernel size k배 만큼 더 비싸다. 그래도 separable convolution을 사용하면 복잡도를 상당히 줄일 수 있고 $$k = n$$일 경우 그 복잡도는 self-attention layer + point-wise feed-forward layer 조합과 비슷해진다.

# 7. Conclusion

이 논문은 Transformer라는 아키덱처를 제안한다. 이것은 Attention에만 기반한 최초의 sequence transduction model이다. 기존에 인코더-디코더 아키텍처에서 사용되던 recurrent layers를 multi-headed self-attention으로 대체했다.

그 결과로 기존에 RNN이나 CNN보다 학습이 훨씬 빨라졌고 더 좋은 성능을 보여준다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
