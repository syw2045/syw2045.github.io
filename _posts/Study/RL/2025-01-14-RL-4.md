---
title: "[RL] David Silver RL Lecture 5: Model-Free Control"

categories:
  - RL
tags:
  - [RL]

toc: true
toc_sticky: true

date: 2025-01-14
last_modified_at: 2025-01-14
---
# 1. Introduction

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” Model-Free controlì— ëŒ€í•´ì„œ ë°°ìš´ë‹¤.   
MDPì— ëŒ€í•œ ì •ë³´ë¥¼ ëª¨ë¥´ëŠ” ìƒí™©(í™˜ê²½ì„ ì•Œì§€ ëª»í•˜ëŠ” ìƒí™©)ì—ì„œ ë˜ì ¸ì§„ Agentë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ ìµœì ì˜ value function, Poilcyë¥¼ ì°¾ì„ ê²ƒì¸ê°€?

## 1.1 Uses of Model-Free Control

ëª‡ ê°€ì§€ ì˜ˆì‹œ ë¬¸ì œë“¤ì€ MDPs ëª¨ë¸í™” ë  ìˆ˜ ìˆë‹¤.
- Elevator, Robocup Soccer
- Helicopter, Robot Walking ë“±ë“±

ëŒ€ë¶€ë¶„ì˜ ì´ëŸ¬í•œ ë¬¸ì œë“¤ì€
- MDP ëª¨ë¸ì„ ëª¨ë¥´ì§€ë§Œ, experienceë¥¼ sampling í•  ìˆ˜ ìˆë‹¤.
- MDP ëª¨ë¸ì„ ì•Œì•„ë„, sampling ì™¸ì— ì‚¬ìš©í•˜ê¸°ì— ë„ˆë¬´ í¬ë‹¤

â‡’ Model-free controlì€ ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ í’€ ìˆ˜ ìˆë‹¤.

## 1.2 On and Off-Policy Learning
- On-policy learning
    - â€œLearn on the jobâ€
    - Learn about policy $$Ï€$$ from experience sampled from $$Ï€$$
    - optimizeí•˜ê³  ì‹¶ì€ policy(target policy) = behavior policyì¼ ë•Œ
    (experienceë¥¼ í•˜ëŠ” policyì™€ optimize í•˜ëŠ” policyê°€ ê°™ì„ ë•Œ)
- Off-policy learning
    - â€œLook over someoneâ€™s shoulderâ€
    - Learn about policy $$Ï€$$ from experience sampled from $$Âµ$$
    - ë‹¤ë¥¸ ì‚¬ëŒì´ ë§Œë“œëŠ” experienceë¥¼ í†µí•´ policy optimize
    
[ì°¸ê³  : On/Off Policy](https://syw2045.github.io/terminology/RL-0/)

---

# 2. On-Policy Monte-Carlo Control

## 2.1 Monte-Carlo Control

![image](https://github.com/user-attachments/assets/492d3d90-6769-42de-9f09-8c0bbba3440f)

Policy Iterationì˜ ì›ë¦¬    
- policy evaluationê³¼ policy improvementë¥¼ ë°˜ë³µí•´ì„œ ìˆ˜í–‰í•˜ë©´ì„œ ì ì°¨ ê°œì„ ëœ policyë¥¼ ì°¾ì•„ë‚´ëŠ” ê³¼ì • â‡’ Control ë¬¸ì œë¥¼ í‘¼ë‹¤
- Policy evaluation : Policy í‰ê°€
    - ex) Iterative policy evaluation
- Policy improvement : í‰ê°€ëœ Policyë¥¼ í†µí•´ ê°œì„ ëœ Policy ìƒì„±
    - ex) Greedy policy improvement


Model-Free ì—ì„œë„ Model-basedì—ì„œ ì‚¬ìš©í•œ(Iterative, greedy)ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ë˜ëŠ” ê±° ì•„ë‹ê¹Œ?

> MDPë¥¼ ëª¨ë¥¼ ë•Œ ë¬¸ì œì  ë° í•´ê²°ë°©ì•ˆ

1. Iterative policy evaluationì„ ì ìš©í•  ìˆ˜ ì—†ë‹¤.

    - Iterative policy evaluationì€ Bellman Expectation Equationì„ ê¸°ë°˜ìœ¼ë¡œ Valueë¥¼ ê³„ì‚°í•´ë‚˜ê°€ëŠ” ê³¼ì •

        ![image 2](https://github.com/user-attachments/assets/d8c1a5e6-64c6-403e-9815-d6c01215c79f)

    - í•˜ì§€ë§Œ Bellman Expectation Equationì—ëŠ” MDPë¥¼ ì•Œì•„ì•¼ë§Œ ì•Œ ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— MDPë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” Model-Free í™˜ê²½ì—ì„œëŠ” ì´ ì‹ì„ ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ì—†ë‹¤.

        - State sì—ì„œ Action aë¥¼ í–ˆì„ ë•Œ ë°›ëŠ” rewardëŠ” Agentê°€ ê²½í—˜í•´ë´ì•¼ ì•Œ ìˆ˜ ìˆìŒ
        - state transitionë„ Agentê°€ ê²½í—˜í•´ë´ì•¼ ì•Œ ìˆ˜ ìˆìŒ, ê²½í—˜ì„ í•˜ë”ë¼ë„ êµ¬ì²´ì ì¸ í™•ë¥  ë¶„í¬ë¥¼ ì •ì˜í•˜ì§€ ëª»í•œë‹¤.
    
    ![image 1](https://github.com/user-attachments/assets/ecbae59b-aaf5-4bd1-950c-3d13ffb19806)

    - Iterative policy evaluation â‡’ <u>Monte-Carlo Policy evaluationìœ¼ë¡œ ì§„í–‰í•œë‹¤.</u>


2. Vë¥¼ ê¸°ë°˜ìœ¼ë¡œëŠ” Greedy Policy improvementì„ í•  ìˆ˜ê°€ ì—†ë‹¤.

    - Greedy policyëŠ” state-value functionì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ valueê°€ ë†’ì€ next-stateì— ë„ë‹¬í•˜ê²Œ í•˜ëŠ” actionì„ ì„ íƒí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤.

    - ê·¸ëŸ°ë° model-free í™˜ê²½ì—ì„œëŠ” state transitionë„ ì‹¤ì œ ê²½í—˜ì„ í•´ë´ì•¼ ì•Œ ìˆ˜ ìˆê³ , êµ¬ì²´ì ì¸ í™•ë¥  ë¶„í¬ë¡œ ì •ì˜í•˜ì§€ ëª»í•œë‹¤.

    - ë”°ë¼ì„œ ì–´ë–¤ actionì„ í–ˆì„ ë•Œ ì´ë™í•˜ëŠ” state $$s'$$ ì—ì„œì˜ value functionê°’ì´ ë†’ì€ ê³³ì„ ì„ íƒí•˜ëŠ” greedy policyë¥¼ ì •ì˜í•  ìˆ˜ ì—†ë‹¤.


![image 3](https://github.com/user-attachments/assets/6a04d0ef-ceb4-4a8d-827e-1cfa13c994e5)

- MDP Modelì„ ì•Œì•„ì•¼ stateë§ˆë‹¤ì˜ value function : Vì— ëŒ€í•œ greedy improvementë¥¼ í•  ìˆ˜ ìˆë‹¤.
- Policy Evaluation ë‹¨ê³„ì—ì„œ actionì— ëŒ€í•œ Q(s,a)ë¥¼ í•™ìŠµí–ˆë‹¤ê³  í•˜ë©´, <u>Qì— ëŒ€í•œ greedy improvementëŠ” ê°€ëŠ¥í•˜ë‹¤.</u>

![image 4](https://github.com/user-attachments/assets/6600d9c3-f138-4950-93f6-889d9af0e61d)

Greedy policy improvement : ëª¨ë“  State ë“¤ì„ ì¶©ë¶„íˆ ë´ì•¼ ëœë‹¤. <u>í•˜ì§€ë§Œ Greedy í•˜ê²Œ í•˜ë©´ explorationì´ ì¶©ë¶„íˆ ë˜ì§€ ì•ŠëŠ”ë‹¤. (ìƒˆë¡œìš´ ë¬¸ì œì )</u>

> Greedy policy improvement (ë¬¸ì œì )

![image 5](https://github.com/user-attachments/assets/0a7cf5fa-dd4c-46e7-b03f-f1ddde6354c6)

ë§¨ ì²˜ìŒ ì™¼ìª½ ë¬¸ì„ ì—´ê³  reward 0ì„ ë°›ìŒ, ê·¸ ë‹¤ìŒ ì˜¤ë¥¸ìª½ ë¬¸ì„ ì—´ê³  reward 1ì„ ë°›ìŒ   
â‡’ ë‹¤ìŒ ë¬¸ì„ ì„ íƒí•  ë•Œ Greedy í•˜ê²Œ ìƒê°í•˜ë©´ ì˜¤ë¥¸ìª½ì„ ì—´ì–´ì•¼í•œë‹¤.   
    ê³„ì† ê·¸ë ‡ê²Œ ì˜¤ë¥¸ìª½ ë¬¸ë§Œ ì—¬ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì •ë§ ìµœì ì˜ í–‰ë™ì„ ì„ íƒì„ í•œ ê²ƒì¼ê¹Œ?

- Explorationì´ ê³ ë ¤ë˜ì–´ì•¼í•˜ëŠ” ì´ìœ 
    
    model-basedì™€ ë‹¬ë¦¬ model-freeí™˜ê²½ì—ì„œëŠ” ì‹¤ì œë¡œ Agentê°€ actionì„ ì„ íƒí•˜ê³  stateì‚¬ì´ë¥¼ ì´ë™í•˜ë©´ì„œ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°°ìš°ê²Œ ëœë‹¤.   
    ê·¸ëŸ°ë° í•™ìŠµì˜ ì´ˆê¸° ë‹¨ê³„ì—ì„œë¶€í„° greedy actionë§Œì„ ì„ íƒí•œë‹¤ë©´ Agentê°€ ë‹¤ì–‘í•œ stateë¥¼ ë°©ë¬¸í•˜ì§€ ëª»í•˜ê³  ì´ë¯¸ ë°©ë¬¸í•œ stateë§Œ ê³„ì† ë°©ë¬¸í•˜ê²Œ ë  ìˆ˜ ìˆë‹¤.
    

> $$Îµ$$-Greedy Exploration (í•´ê²°ì±…)

- $$1-Îµ$$ì˜ í™•ë¥ ë¡œ greedyí•œ action ì„ íƒ (Exploitation)
- $$Îµ$$ì˜ í™•ë¥ ë¡œ ëœë¤í•œ action ì„ íƒ (Exploration)

ì¥ì  

1. policyê°€ ê°œì„ ë¨ì„ ë³´ì¥í•  ìˆ˜ ìˆë‹¤.
2. ëª¨ë“  actionì„ explor í•œë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•  ìˆ˜ ìˆë‹¤
    
    ![image 6](https://github.com/user-attachments/assets/c4b3e4ef-16b2-4aaf-9f34-8fba1600db89)
    

Policy ê°œì„ ì— ëŒ€í•œ ì¦ëª…

![image 7](https://github.com/user-attachments/assets/34b9e377-1f9b-46c1-9f90-e354ae537ca9)

 

![image 8](https://github.com/user-attachments/assets/7b64e701-c4df-4741-a5ba-73ba4cf41752)

![image 9](https://github.com/user-attachments/assets/77d5b965-3f49-4b38-b7b6-5c7b7425b9be)

í•œ episodeê°€ ëë‚˜ë©´ í•œ episodeë§Œí¼ì˜ ê²½í—˜ì„ ê°€ì§€ê³  policyë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‹¤.

<u>Që¡œ ì‹œì‘ì„ ì‹œì‘í–ˆì„ë•Œ optimally í•˜ê²Œ ìˆ˜ë ´ì„ í•˜ëŠ”ê°€?</u>

â‡’ ì´ë ‡ê²Œ í•  ë•Œ ì˜ ìˆ˜ë ´í•˜ê¸° ìœ„í•œ ì„±ì§ˆì´ ì¡´ì¬í•œë‹¤.

## 2.2 GLIE Property

![image 10](https://github.com/user-attachments/assets/d8139c60-36b5-403e-b6b3-1cbae9537c97)

- ëª¨ë“  state-action pairë“¤ì´ ë¬´í•œíˆ ë§ì´ explored ë˜ì–´ì•¼ í•œë‹¤.
    - $$Îµ$$-greedyì´ê¸° ë•Œë¬¸ì— ë¬´í•œíˆ ë§ì€ stateë¥¼ ì¶©ë¶„íˆ explore ê°€ëŠ¥
- ê²°êµ­ì— policyëŠ” greedy policyë¡œ ìˆ˜ë ´í•´ì•¼ í•œë‹¤. (exploitation)

![image 11](https://github.com/user-attachments/assets/9e75ed2e-5ecb-4a49-aa3a-70ecab6b5232)
- <u>GLIE Monte-Carlo controlì€ optimal action-value functionìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤.</u>

---

# 3. On-Policy Temporal-Difference Learning   

Temporal-difference(TD) learning ì€ Monte-Carlo(MC)ì— ë¹„í•´ ì¥ì ë“¤ì´ ìˆë‹¤.

- ë‚®ì€ ë¶„ì‚°
- Online (episodeê°€ ì•ˆëë‚˜ë„ í•™ìŠµ ê°€ëŠ¥)
- Incomplete sequencesì—ë„ ì ìš©ê°€ëŠ¥

â‡’ <u>Control loopì—ì„œ MCëŒ€ì‹ ì— TDë¥¼ ë„£ì„ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?</u>

- Q(S,A)ì— TDë¥¼ ì ìš©í•˜ê³ 
- $$Îµ$$-greedy policy improvementë¥¼ ì‚¬ìš©í•˜ê³ 
- ë§¤ time-stepë§ˆë‹¤ updateë¥¼ í•œë‹¤.

## 3.1 SARSA

![image 12](https://github.com/user-attachments/assets/4e53b0f4-1278-4e12-8d2e-6799fdbab5ec){: width="30%", height="30%"}

SARSA: stateÂ Sì—ì„œ actionÂ Aë¥¼ ìˆ˜í–‰í•˜ì—¬ rewardÂ Rì„ ë°›ê³  next stateÂ S'ì— ë„ë‹¬í•œ ë’¤, ë‹¤ì‹œ actionÂ A'ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •

TDì´ê¸° ë•Œë¬¸ì— episodeê°€ ì•ˆëë‚˜ë„ ë°”ë¡œ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.

One-step actionì„ í•˜ê³ , rewardë¥¼ ë°›ê³  Q(S,A)ìë¦¬ë¥¼ updateí•œë‹¤.

![image 13](https://github.com/user-attachments/assets/065e6f5c-8ab6-41db-99bf-b6716d0bd65e)

ì „ ìŠ¬ë¼ì´ë“œëŠ” Every episode ì˜€ëŠ”ë° Every time-stepìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤.

- Policy evaluationë‹¨ê³„ì—ì„œëŠ” MC ëŒ€ì‹  Sarsaë¡œ Që¥¼ evaluation
- Policy improvementë‹¨ê³„ì—ì„œëŠ” $$Îµ$$-greedy policy improvement

## 3.2 SARSA Algorithm

![image 14](https://github.com/user-attachments/assets/73beb901-b74f-4a89-b90d-a35b379f071f)

1. ì´ˆê¸°ì—ëŠ” ëœë¤í•œ ê°’ë“¤ë¡œÂ Q tableì„ ì´ˆê¸°í™”í•œë‹¤.
2. Qì— ëŒ€í•œÂ $$Ïµ$$-greedyë°©ë²•ì„ ì´ìš©í•˜ì—¬ stateÂ *S*ì—ì„œì˜ actionÂ *A*ë¥¼ ê³ ë¥¸ë‹¤.
3. actionÂ *A*ë¥¼ Agentê°€ ì‹œí–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¡œ ë°›ê²Œë˜ëŠ” rewardÂ *R*ê³¼ ë„ë‹¬í•œ next-stateÂ *S'*ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°›ëŠ”ë‹¤.
4. *Q*ì— ëŒ€í•œÂ $$Ïµ$$-greedyë°©ë²•ì„ ì´ìš©í•˜ì—¬ ì´ë™í•œ stateÂ $$S'$$ì—ì„œì˜ actionÂ $$A'$$ë¥¼ ì„ íƒí•œë‹¤.
5. next state-action pairì— ëŒ€í•œ Q-valueë¥¼ ì´ìš©í•˜ì—¬ current Q-valueë¥¼Â **TD**ë°©ë²•ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤.
6. current state, actionì—Â $$S'$$,Â $$A'$$ë¥¼ ëŒ€ì…í•œë‹¤.

![image 15](https://github.com/user-attachments/assets/37a36c55-e9d0-4157-9819-daadbb1a1684)

Sarsa ëŠ” optimal action-value functionì— ìˆ˜ë ´í•œë‹¤.

- GLIE í•´ì•¼í•œë‹¤.
    - ëª¨ë“  State-action pairë¥¼ ë°©ë¬¸í•œë‹¤.
    - ê²°êµ­ì€ greey policyë¡œ ìˆ˜ë ´í•œë‹¤.
- Robbins-Monro sequence
    - $$Î±$$ë¥¼ ë¬´í•œíˆ ë”í•˜ë©´ $$âˆ$$ (Î±ê°€ Që¥¼ ë¬´í•œí•œ ê³³ê¹Œì§€ ë°ë ¤ê°ˆ ìˆ˜ ìˆë„ë¡ í•´ì•¼í•œë‹¤.)
    - $$Î±^2$$ì˜ í•©ì´ $$âˆ$$ë³´ë‹¤ ì‘ë‹¤. (Q valueë¥¼ ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ì ì  ì‘ì•„ì ¸ì„œ ìˆ˜ë ´í•˜ê²Œ í•œë‹¤.)
    

â‡’ ì‹¤ì§ˆì ìœ¼ë¡œ ë‘ ì¡°ê±´ì„ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹˜ ì‹¤ì œë¡œëŠ” ì˜ ìˆ˜ë ´í•˜ë”ë¼

## 3.3 n-step SARSA(ğº)

![image 16](https://github.com/user-attachments/assets/ad942514-a030-42db-ab87-e180316419e1)

- në§Œí¼ì˜ ì‹¤ì œ rewardì™€, t+në²ˆì§¸ stepì—ì„œì˜ ì¶”ì • value functionì˜ í•©ìœ¼ë¡œ í‘œí˜„í•œë‹¤.

![image 17](https://github.com/user-attachments/assets/b6ef1024-1c7a-41bf-8cd9-eb364cc95c9d)

- episodeê°€ ëë‚˜ì•¼ ì ìš©ì´ ê°€ëŠ¥í•œ Foward-view Sarsa(ğº)

![image 18](https://github.com/user-attachments/assets/1a99fe58-8b80-4da4-a292-76798fd6058a)

- TD(*Î»*)ì—ì„œ ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼ SARSAë„ eligibility tracesë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤.
- ë‹¨, SARSAëŠ” Q-functionì— ëŒ€í•´ TDë¥¼ ì ìš©í•˜ë¯€ë¡œ ê°ê°ì˜ state-action pairì— ëŒ€í•´ ëŒ€ì‘ë˜ëŠ” í•˜ë‚˜ì˜ eligibility traceë¥¼ ê°€ì§„ë‹¤.

eligibility trace

- init :Â $$E_0(s,a)$$=0
- time-stepÂ $$t$$ì—ì„œ ì–´ë–¤ stateÂ $$s$$ì—ì„œ ì–´ë–¤ actionÂ $$a$$ë¥¼ ìˆ˜í–‰í•˜ë©´,Â 1ì„ ë”í•´ì£¼ê³  ë°©ë¬¸í•˜ì§€ ì•Šì•˜ì„ ë•ŒëŠ”Â $$t$$âˆ’1ì—ì„œì˜ ê°’ì—ë‹¤ê°€Â $$Î³âˆˆ(0,1)$$ë¥¼ ê³±í•´ì¤˜ì„œ ê°’ì„ ê°ì†Œì‹œí‚¨ë‹¤.

![image 19](https://github.com/user-attachments/assets/ebe9b61c-acfb-4b2b-a2e6-120a983b2a89)

ì›ë˜ëŠ” í•œ stateì—ì„œ actionì„ í•œ ë’¤ ê·¸ ì¹¸ë§Œ updateí•´ì£¼ì—ˆì§€ë§Œ, Sarsa(ğº)ëŠ” ê³¼ê±°ì˜ ì§€ë‚˜ê°”ë˜ ì¹¸ë„ Eligibility traces ê°’ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ëª¨ë“  stateë¥¼ updateê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ì§€ë§Œ ì •ë³´ì „íŒŒê°€ ë¹ ë¥´ë‹¤.

![image 20](https://github.com/user-attachments/assets/8d660f03-76dc-4406-b535-a7068d9caf79)

- one-step SARSAë¥¼ í•˜ë©´ ë„ì°©í•˜ëŠ” ìˆœê°„ rewardë¥¼ ë°›ê³ , ì´ì „ stateë§Œ update
- Sarsa(ğº)ëŠ” ì˜¤ê¸°ê¹Œì§€ ê±°ì³¤ë˜ ëª¨ë“  ê²½ë¡œë“¤ì— ëŒ€í•´ Eligibility traces ê°’ë§Œí¼ reward update
    - ìµœê·¼ì˜ ê²½ë¡œì¼ ìˆ˜ë¡ ë§ì´ updateëœë‹¤.

---

# 4. Off-Policy Learning

## 4.1 Off-policy learning

off-policy ë€?

target policy $$Ï€$$ë¥¼ ë”°ëì„ ë•Œì˜ Valueë¥¼ ê³„ì‚°í•˜ê±°ë‚˜, policyë¥¼ ê°œì„ í•˜ê³  ì‹¶ì„ ë•Œ targetê³¼ ë‹¤ë¥¸ behavior policy $$Âµ$$ë¥¼ ë”°ëì„ ë•Œì˜ ê²½í—˜ì  ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•

- target policyÂ $$Ï€(aâˆ£s)$$ : compute $$v_Ï€(s)$$ or $$q_Ï€(s,a)$$
    - actionì„ sampling í•˜ëŠ” policy
- behavior policyÂ $$Î¼(aâˆ£s)$$ : {$${S_1,A_1,R_2,â‹¯,S_T}$$ } ~ $$Î¼$$
    - ê°œì„ í•˜ê³  ì‹¶ì€ policy
    
- off-policyì˜ ì¥ì 
    - ì‚¬ëŒì´ë‚˜ ë‹¤ë¥¸ Agentsë¥¼ ê´€ì°°í•˜ë©´ì„œ ë°°ìš¸ ìˆ˜ ìˆë‹¤.
    - old policies ì¸ $$Ï€_1$$ , $$Ï€_2$$ â€¦ $$Ï€_t$$ ê°€ ìƒì„±í•œ ê²½í—˜ë„ ì¬ì‚¬ìš© í•  ìˆ˜ ìˆë‹¤.
    - exploratory policyë¥¼ ë”°ë¥´ë©´ì„œ optimal policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
    - í•˜ë‚˜ì˜ policyë¥¼ ë”°ë¥´ë©´ì„œ ì—¬ëŸ¬ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

## 4.2 Importance Sampling for off-policy

- Importance Sampling : `ë‘ í™•ë¥ ë¶„í¬ì˜ ë¹„ìœ¨`ë§Œ ê³±í•´ì£¼ë©´ ì–´ë–¤ í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í•œ ê°’ì— ëŒ€í•œ ê¸°ëŒ“ê°’ì„ ë‹¤ë¥¸ í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í–ˆì„ ë•Œì— ëŒ€í•œ ê¸°ëŒ“ê°’ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

![image 21](https://github.com/user-attachments/assets/a1b14a7f-ee7d-400b-a82c-68af0573e367)

Pë¥¼ ë”°ë¥¼ ë•Œì˜ f(X)ì˜ ê¸°ëŒ“ê°’ = Î£ P(X) * f(X)

â†’ ë¶„ëª¨,ë¶„ìì— Q(X) ê³±í•œë‹¤.

â†’ Që¥¼ ë°–ìœ¼ë¡œ ë¹¼ë‚¸ë‹¤.

â†’ Që¥¼ ë”°ë¥¼ ë•Œì˜ ê¸°ëŒ€ê°’ = {P(X) / Q(X)} * f(X)

â‡’ <u>P í™•ë¥  ë¶„í¬ ê¸°ë°˜ì˜ ê¸°ëŒ“ê°’ì„ Q í™•ë¥  ë¶„í¬ ê¸°ë°˜ì˜ ê¸°ëŒ€ê°’ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤.</u>

![image 22](https://github.com/user-attachments/assets/1e8625b9-359c-40a9-b3b2-af5069444453)

- $$G_t$$ë¥¼ ì–»ì„ ë•Œê¹Œì§€ ìˆ˜í–‰í•œ ê°ê°ì˜ actionì´ 'ì„ íƒë  í™•ë¥ ì˜ ë¹„'ë¥¼ ê³„ì† ê³±í•´ì¤€ë‹¤.

- ê·¸ëŸ¬ë‚˜ MCëŠ” ì „ì²´ episodeê°€ ëë‚œ ë‹¤ìŒì— $$G_t$$ë¥¼ ë°›ì•„ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì—, ëë‚  ë•Œê¹Œì§€ ìˆ˜í–‰í•œ actionì˜ ìˆ˜ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ $$G_t$$ì˜ ì•ì— ê³±í•´ì§€ëŠ” ratioê°€ ë„ˆë¬´ ë§ì•„ì§€ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ì´ ë°©ë²•ì„ ì¨ì„œ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤. (ratioë“¤ì˜ ê³±ì— ëŒ€í•œ varianceê°€ ë„ˆë¬´ í¬ë‹¤)

â‡’ <u>1-step í›„ì— updateê°€ ê°€ëŠ¥í•œ TDëŠ” ì–´ë–¨ê¹Œ?</u>

![image 23](https://github.com/user-attachments/assets/e5cbb0fc-868b-48c8-a48c-7692ba75a1c1)

<u>TDì˜ ê²½ìš°ì—ëŠ” ì•ì— ê³±í•´ì§€ëŠ” ìˆ˜ê°€ í›¨ì”¬ ì ê¸° ë•Œë¬¸ì— Importance samplingì„ ì´ìš©í•˜ì—¬ off-policy methodë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</u>

## 4.3 Q-Learning

- Importance samplingì„ ì•ˆì“°ê³  action value Q(s,a)ì„ off-policyë¡œ í•™ìŠµí•˜ê³  ì‹¶ë‹¤.
- Agentê°€ ì‹¤í–‰í•  ì‹¤ì œ next-actionì€ behavior policyë¥¼ ë”°ë¼ ì„ íƒëœë‹¤.   $$A_{t+1}âˆ¼Î¼(â‹…âˆ£S_t)$$   
- ê·¸ëŸ¬ë‚˜ Q-functionì„ updateí•  ë•ŒëŠ” target policyë¥¼ ë”°ë¼ ì„ íƒëœ actionÂ $$A'$$ì— ëŒ€í•˜ì—¬ ê³„ì‚°í•œë‹¤.  $$A'âˆ¼Ï€(â‹…âˆ£S_t)$$   
- TDëŠ” reward + ì¶”ì¸¡ê°’ì˜ ì°¨ì´ë¥¼ ì´ìš©í•˜ì—¬ í˜„ì¬ê°’ì„ updateí•˜ëŠ”ë°, ì¶”ì¸¡ê°’ì—ì„œëŠ” behavior policyë¥¼ ë”°ë¥´ì§€ ì•Šì•„ë„ ìƒê´€ì—†ê¸° ë•Œë¬¸

![image 24](https://github.com/user-attachments/assets/29523412-0ead-463b-ade5-944d39d7ba68)

- $$A_{t+1}$$ì´ ì•„ë‹ˆë¼ $$A'$$ì„ ê°€ì ¸ì™€ì„œ $$Q$$($$S_t$$, $$A_t$$)ë¥¼ Update â‡’ Off-policy
- <u>Actionì€ behavior policy Î¼ì—ì„œ ì„ íƒì„ í•˜ê³ , target policy Ï€ì—ì„œì˜ Qê°’ì„ update í•œë‹¤.</u>

## 4.4 Off-Policy Control with Q-Learning

- behavior policyì™€ target policy ë‘˜ë‹¤ ì ì°¨ improvementê°€ ë˜ì§€ë§Œ, behavior policyëŠ” ì—¬ì „íˆ explorationì„ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ policyë¥¼ ì„¤ì •í•˜ê³  ì‹¶ë‹¤.

- Target policy $$Ï€$$ : Greedy

![image 25](https://github.com/user-attachments/assets/d99de03e-7dda-4e45-b2ef-83020071f313)

- behavior policy $$Î¼$$ : $$Îµ$$-Greedy

![image 26](https://github.com/user-attachments/assets/ec663734-982b-49b5-9ae5-45490b14bbd7)

target policyë¥¼ ì´ìš©í•˜ì—¬ ì„ íƒëœ action $$A'$$ë¥¼ greedy policyì— ëŒ€í•œ ìˆ˜ì‹ìœ¼ë¡œ ë°”ê¾¸ì–´ì„œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. â‡’ Q-Learningì—ì„œì˜ TD targetì´ ëœë‹¤

Q-Learning Update ì‹ : 

![image 27](https://github.com/user-attachments/assets/e524ba26-da38-42c2-88c8-fe23a7a3082e)

- maxê°€ ì¶”ê°€ëë‹¤.
- Sarsa maxë¼ê³ ë„ ë¶ˆë¦°ë‹¤.

![image 28](https://github.com/user-attachments/assets/5a1e9664-87f1-4f96-aacd-95a3edd4726b)

# Summary

![image 29](https://github.com/user-attachments/assets/23319b39-e88f-4f57-83cc-7d9013aa32b8)

[ë§¨ ìœ„ë¡œ ì´ë™í•˜ê¸°](#){: .btn .btn--primary }{: .align-right}