---
title: "On-policy/Off-policy"

categories:
  - Terminology
tags:
  - [RL]

toc: true
toc_sticky: true

date: 2024-12-30
last_modified_at: 2025-01-14
---
# 1. On-policy Off-policy

On-policy/Off-policy에 대해서 이해를 쉽게 하기 위해 예를 들어보면

스타크래프트를 배울 때 **내가 직접 플레이**하면서 이기고 지는 것을 반복해 **배울 수 있다**.   
하지만 이와 반대로 **친구가 하는 것을 뒤에서 보고** ‘아 지금은 멀티를 먹지 말고 타이밍 러쉬를 갔어야지’를 **깨우칠 수도 있다**.

전자가 On-policy, 후자가 Off-policy이다.

강화학습 알고리즘은 On-policy 방식과 Off-policy 방식으로 분류될 수 있다.

- On-policy : `Behavior Policy` = `Target Policy`
- Off-policy : `Behavior Policy` ≠ `Target Policy`

Behavior Policy : 데이터를 수집하기 위한 실제 행동 규칙, exploration과 같은 행동의 다양성을 제공   
Target Policy : 학습의 목표로, 환경에서 최적의 보상을 얻기 위한 최적화된 정책

> 데이터 수집

- 에이전트가 환경과 상호작용하여 “경험”을 얻는 과정이다.
- 어떤 상태 $$s_t$$에서 행동 $$a_t$$를 선택하고, 결과적으로 새로운 상태 $$s_{t+1}$$과 보상 $$r_t$$을 얻는다.
- 이런 경험을 **Transition**이라고 한다 $$(s_t, a_t, r_t, s_{t+1})$$

> 학습

- 수집된 데이터를 바탕으로, Target Policy를 업데이트 하는 과정
- 수집된 Transition 데이터를 이용해 가치 함수 (Q 함수) 또는 정책(Policy)를 개선
- 이때 학습 방법에 따라 On-policy와 Off-policy로 나뉜다.

---

## 1.1 On-policy

> 정의

- 직접 스타크래프트를 하면서 학습하는 예시처럼, 데이터 수집 (Behavior Policy)과 학습(Target Policy)이 동일한 정책을 따른다.
- On-policy 학습에서는 매번 정책이 업데이트되면서 Q 값(state-action value function)도 변경된다
→ 이전 정책으로 수집한 데이터는 더 이상 현재 정책에 적합하지 않게 된다.

> 문제점

- 이로 인해 과거 데이터를 재사용할 경우, 현재의 정책과 불일치한 데이터를 학습하게 되어 문제가 발생
- 따라서 수집된 데이터를 재사용하지 않고 **한 번 사용한 뒤 버리는 방식**을 채택하게 된다.
- 수집된 데이터를 반복적으로 학습하지 못하므로, 데이터 효율성이 떨어질 수 있다.
- 예를 들어, 한 번의 플레이에서 얻은 경험(Transition)은 학습 시 바로 소모되고 다시 수집해야 하므로, 경험 데이터가 충분히 활용되지 않는다.

> 장점

- Low bias error : Behavior Policy와 Target Policy가 같으므로 일반적으로 bias error를 유발시키지 않아 성능이 안정적이다.

> 단점

- Low sample efficiency : 획득한 데이터를 이용해 Policy를 한번 업데이트하고 폐기하므로 환경과 상호작용이 많이 필요하다.

> **대표 알고리즘: SARSA**

- ε-greedy를 사용하여 데이터를 수집(Behavior Policy).
- ε-greedy를 학습 대상(Target Policy)으로 사용.
- 결과적으로, Behavior Policy와 Target Policy가 동일하기 때문에, 매번 **현재 정책으로 데이터를 수집하고 현재 정책을 학습**합니다.

## 1.2 Off-Policy

> 정의

- **Behavior Policy**(데이터 수집 정책)와 **Target Policy**(학습 목표 정책)가 다르다.
- Behavior Policy: ε-greedy (exploration을 위해 무작위 행동 포함)
- Target Policy: greedy (항상 최적 행동 선택)

에이전트는 ε-greedy 정책으로 행동하며 데이터를 수집하지만, 학습 과정에서는 최적 정책(greedy)을 학습.   
이를 통해 Behavior Policy로 다양한 데이터를 수집하면서도 Target Policy를 향해 수렴한다.

> 문제점

- 학습하는 정책과 데이터 생성 정책이 다르기 때문에 안정성이 떨어질 수 있다.
- 잘못된 데이터(비정확하거나 오래된 데이터)가 들어오면 학습 성능에 악영향을 미칠 수 있다.

> 장점

- High sample efficiency : 과거의 policy로 부터 획득한 데이터를 현재 policy를 업데이트할 때 여러번 재사용이 가능하고 환경과 상호작용을 적게할 수 있다.

> 단점

- High bias error : 과거의 Policy와 현재의 Policy가 많이 달라진 경우, 과거의 데이터는 현재의 Policy를 업데이트 하기에 좋은 데이터가 아닐 수 있다.

> 대표 알고리즘: Q-Learning

- Behavior Policy: ε-greedy를 사용하여 데이터를 수집(exploration을 위해 무작위 행동을 일부 포함).
- Target Policy: greedy 정책(항상 최적 행동)을 학습 목표로 사용.
- 여기서 에이전트는 ε-greedy로 수집한 데이터로 greedy 정책을 학습하므로 Behavior Policy ≠ Target Policy이다.

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
